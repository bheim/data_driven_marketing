---
title: "Advertising Effects"
author: "Ben Heim, Marcell Milo-Sidlo, Pierre Chan, Julia Luo, Alicia Soosai"
output:
  pdf_document:
    number_sections: yes
    toc: yes
header-includes: \usepackage{booktabs}
urlcolor: blue
graphics: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE,
                      fig.width = 4.5, fig.height = 3, fig.align = "right")
```

\setlength{\parskip}{6pt}
\newpage




```{r}
library(bit64)
library(data.table)
library(RcppRoll)
library(ggplot2)
library(fixest)
library(knitr)
```

\bigskip




# Overview

In this assignment we estimate the causal short and long-run effects of advertising on demand. The assignment is closely related to the paper "TV Advertising Effectiveness and Profitability: Generalizable Results from 288 Brands" (2001, *Econometrica*) by Shapiro, Hitsch, and Tuchman.

We first combine store-level sales data from the Nielsen RMS scanner data set with DMA-level advertising exposure data from the Nielsen Ad Intel advertising data set. We then estimate ad effects based on a within-market strategy controlling for cross-sectional heterogeneity across markets.




# Data

## Brands and product modules

Data location:

```{r}
data_folder = "~/Desktop/data_driven_marketing/ddm_assignments/hw3/data"
```

\bigskip

The table `brands_DT` in the file `Brands_a3.RData` provides information on the available product categories (product modules) and brands, including the "focal" brands for which we may estimate advertising effects.

```{r}
brands = load(paste0(data_folder, "/Brands_a3.RData"))
```

\medskip

```{}
 product_module_code       product_module_desc   brand_code_uc        brand_descr  focal_brand
                1484   SOFT DRINKS - CARBONATED         531429        COCA-COLA R         TRUE
                8412                   ANTACIDS         621727           PRILOSEC         TRUE
                1553  SOFT DRINKS - LOW CALORIE         531433  COCA-COLA ZERO DT         TRUE
```

\bigskip

Choose Prilosec in the Antacids category for your analysis. Later, you can **optionally** repeat your analysis for the other brands.

```{r}
selected_module = 8412
selected_brand  = 621727
```

\newpage


# 3 Data preparation

To prepare and build the data for the main analysis, load the brand and store meta-data in `Brands_a3.RData` and `stores_dma.RData`, the RMS store-level scanner (movement) data, and the Nielsen Ad Intel DMA-level TV advertising data. The scanner data and advertising data are named according to the product module, such as `move_8412.RData` and `adv_8412.RData`.

Both the RMS scanner data and the Ad Intel advertising data include information for the top four brands in the category (product module). To make our analysis computationally more manageable we will not distinguish among all individual competing brands, but instead we will aggregate all competitors into one single brand.

```{r}
brands = load(paste0(data_folder, "/Brands_a3.RData"))
stores = load(paste0(data_folder, "/stores_dma.RData"))
move_data = load(paste0(data_folder, "/adv_8412.RData"))
advertising_data = load(paste0(data_folder, "/adv_8412.RData"))
```

## 3.1 RMS scanner data (`move`)

Let us start manipulating the `move' dataset.

For consistency, rename the `units` to `quantity` and `promo_percentage` to `promotion` (use the `setnames` command). The promotion variable captures promotional activity as a continuous variable with values between 0 and 1.

Create the variable `brand_name` that we will use to distinguish between the own and aggregate competitor variables. The brand name `own` corresponds to the focal brand (Prilosec in our case), and `comp` (or any other name that you prefer) corresponds to the aggregate competitor brand.

We need to aggregate the data for each store/week observation, separately for the `own` and `comp` data. To aggregate prices and promotions we can take the simple arithmetic `mean` over all competitor brands (a weighted mean may be preferable but is not necessary in this analysis where prices and promotions largely serve as controls, not as the main marketing mix variables of interest). Aggregate quantities can be obtained as the `sum` over brand-level quantities.

```{r}
#Step 1: Renaming (use set names)
#Step 1a: Units -> Quantity 
#Step 1b: promo_percentage -> promotion
library(data.table) 
setnames(move, old = "units", new = "quantity", skip_absent=TRUE)
setnames(move, old = "promo_percentage", new = "promotion", skip_absent=TRUE)
```

```{r}
#Step 2: Identifying 'own' (Prilosec) vs. 'comp' (non-Prilosec)
move$brand_name = ifelse(move$brand_code_uc == selected_brand, "own", "comp")
```

```{r}
#Step 3: Aggregating the data for each store/observation  
#For 'comp'
comp_data = move[move$brand_name == "comp"]
comp_price_mean = mean(comp_data$price, na.rm = TRUE)
comp_promotions_mean = mean(comp_data$promotion, na.rm = TRUE)

#For 'own'
own_data = move[move$brand_name == "own"]
own_price_mean = mean(own_data$price, na.rm = TRUE)
own_promotions_mean = mean(own_data$promotion, na.rm = TRUE)

#Comparison between 'own' and 'comp'
comp_price_mean
comp_promotions_mean 
own_price_mean
own_promotions_mean
```

\bigskip

Later, when we merge the RMS scanner data with the Ad Intel advertising data, we need a common key between the two data sets. This key will be provided by the DMA code and the date. Hence, we need to merge the `dma_code` found in the `stores` table with the RMS movement data.

Now merge the `dma_code` with the movement data.

```{r}
#Merge stores_dma w/move
#Can you check this one because I only see one dma value when I merge both together
dma_stores_merge = merge(move, stores_dma, by = "store_code_uc", all = FALSE)
print(dma_stores_merge)
```


## 3.2 Ad Intel advertising data (`adv_DT`)

The table `adv_DT` contains information on brand-level GRPs (gross rating points) for each DMA/week combination. The original data are more disaggregated, and include individual occurrences on a specific date and at a specific time and the corresponding number of impressions. `adv_DT` is based on the original data, aggregated at the DMA/week level.

Weeks are indicated by `week_end`, where the corresponding date is always a Saturday. We use Saturdays so that the `week_end` variable in the advertising data corresponds to the date convention in the RMS scanner data, where `week_end` also corresponds to a Saturday.

The data contain two variables to measure brand-level GRPs, `grp_direct` and `grp_indirect`. `grp_direct` records GRPs for which we can create a direct, unambiguous match between the brand name in the scanner data and the name of the advertised brand. Sometimes, however, it is not entirely clear if we should associate an ad in the Ad Intel data with the brand in the RMS data. For example, should we count ads for BUD LIGHT BEER LIME when measuring the GRPs that might affect sales of BUD LIGHT BEER? As such matches are somewhat debatable, we record the corresponding GRPs in the variable `grp_indirect`.

\bigskip

The data do not contain observations for all DMA/week combinations during the observation period. In particular, no DMA/week record is included if there was no corresponding advertising activity. For our purposes, however, it is important to capture that the number of GRPs was 0 for such observations. Hence, we need to "fill the gaps" in the data set.

\medskip

data.table makes it easy to achieve this goal. Let's illustrate using a simple example:

```{r}
set.seed(444)
DT = data.table(dma  = rep(LETTERS[1:2], each = 5),
                week = 1:5,
                x    = round(runif(10, min = 0, max =20)))
DT = DT[-c(2, 5, 9)]
DT
```

\medskip

In `DT`, the observations for weeks 2 and 5 in market A and week 4 in market B are missing.

To fill the holes, we need to key the data.table to specify the dimensions---here the `dma` and `week`. Then we perform a *cross join* using `CJ` (see `?CJ`). In particular, for each of the variables along which `DT` is keyed we specify the full set of values that the final data.table should contain. In this example, we want to include the markets A and B and all weeks, 1-5.

```{r}
setkey(DT, dma, week)
DT = DT[CJ(c("A", "B"), 1:5)]
DT
```

\medskip

We can replace all missing values (`NA`) with another value, say -111, like this:

```{r}
DT[is.na(DT)] = -111
DT
```

\bigskip

Use this technique to expand the advertising data in `adv_DT`, using a cross join along along all `brands`, `dma_codes`, and `weeks`:

```{r, eval = FALSE}
brands    = unique(adv_DT$brand_code_uc)
dma_codes = unique(adv_DT$dma_code)
weeks     = seq(from = min(adv_DT$week_end), to = max(adv_DT$week_end), by = "week")
```

Now perform the cross join and set missing values to 0.

```{r}
#Perfoming Cross Join (Double Check)
library(data.table)
setDT(adv_DT)
complete_set = CJ(brand_code_uc = brands, dma_code = dma_codes, week_end = weeks)
filled_DT = complete_set[adv_DT, on = .(brand_code_uc, dma_code, week_end)]
filled_DT[is.na(filled_DT)] = 0
print(filled_DT)
```


\bigskip

Create own and competitor names, and then aggregate the data at the DMA/week level, similar to what we did with the RMS scanner data. In particular, aggregate based on the sum of GRPs (separately for `grp_direct` and `grp_indirect`).

```{r}
#Locating own and comps within advertising datatable...
selected_brand  = 621727
adv_DT[, brand_type := ifelse(adv_DT$brand_code_uc == selected_brand, "own", "comp")]
```

```{r}
#For 'comp'
competitor_data = adv_DT[adv_DT$brand_type == "comp"]
comp_direct_sum = sum(competitor_data$grp_direct)
comp_indirect_sum = sum(competitor_data$grp_indirect)

#For 'own'
owner_data = adv_DT[adv_DT$brand_type == "own"]
owner_direct_mean = sum(owner_data$grp_direct)
owner_indirect_mean = sum(owner_data$grp_indirect)

#Comparison between 'own' and 'comp'
comp_direct_sum
comp_indirect_sum 
owner_direct_mean
owner_indirect_mean
```

\bigskip

At this stage we need to decide if we want to measure GRPs using only `grp_direct` or also including `grp_indirect`. I propose to take the broader measure, and sum the GRPs from the two variables to create a combined `grp` measure. You can later check if your results are robust if you use `grp_direct` only (this robustness analysis is optional).

```{r}
#Direct & Indirect GRPs
grp_comp = comp_direct_sum + comp_indirect_sum 
grp_owner = owner_direct_mean + owner_indirect_mean
```

*Note*: In the Antacids category, `grp_indirect` only contains the value 0 and is therefore not relevant. However, if you work with the data in the other categories, `grp_indirect` contains non-zero values.

\newpage


## Calculate adstock/goodwill

Advertising is likely to have long-run effects on demand. Hence, we will calculate adstock or goodwill variables for own and competitor advertising. We will use the following, widely-used adstock specification ($a_t$ is advertising in period $t$):
$$g_{t} = \sum_{l=0}^{L}\delta^{l}\log(1+a_{t-l}) = \log(1+a_{t})+\delta \log(1+a_{t-1})+\dots+\delta^{L}\log(1+a_{t-L})$$

We add 1 to the advertising levels (GRPs) before taking the log to deal with the large number of zeros in the GRP data.

\medskip

Here is a particularly easy and fast approach to calculate adstocks. First, define the adstock parameters---the number of lags and the carry-over factor $\delta$.

```{r}
N_lags = 52
delta  = 0.7
```

Then calculate the geometric weights based on the carry-over factor.

```{r}
geom_weights = cumprod(c(1.0, rep(delta, times = N_lags)))
geom_weights = sort(geom_weights)
tail(geom_weights)
```

\medskip

Now we can calculate the adstock variable using the `roll_sum` function in the `RcppRoll` package.

```{r, eval = FALSE}
setkey(adv_DT, brand_name, dma_code, week_end)
adv_DT[, adstock := roll_sum(log(1+grp), n = N_lags+1, weights = geom_weights,
                             normalize = FALSE, align = "right",  fill = NA),
        by = .(brand_name, dma_code)]
```

\medskip

Explanations:

1. Key the table along the cross-sectional units (brand name and DMA), then along the time variable. This step is *crucial*! If the table is not correctly sorted, the time-series order of the advertising data will be incorrect.

2. Use the `roll_sum` function based on `log(1+grp)`. `n` indicates the total number of elements in the rolling sum, and `weights` indicates the weights for each element in the sum. `normalize = FALSE` tells the function to leave the `weights` untouched, `align = "right"` indicates to use all data above the current row in the data table to calculate the sum, and `fill = NA` indicates to fill in missing values for the first rows for which there are not enough elements to take the sum.

\medskip

Alternatively, you could code your own weighted sum function:

```{r}
weightedSum <- function(x, w) {
   T = length(x)
   L = length(w) - 1
   y = rep_len(NA, T)
   for (i in (L+1):T) y[i] = sum(x[(i-L):i]*w)
   return(y)
}
```

\bigskip

Let's compare the execution speed:

```{r, eval = FALSE}
time_a = system.time(adv_DT[, stock_a := weightedSum(log(1+grp), geom_weights),
                             by = .(brand_name, dma_code)])
```

```{r, eval = FALSE}
time_b = system.time(adv_DT[, stock_b := roll_sum(log(1+grp), n = N_lags+1,
                                                  weights = geom_weights,
                                                  normalize = FALSE,
                                                  align = "right",  fill = NA),
                             by = .(brand_name, dma_code)])
```

Even though the `weightedSum` function is fast, the speed difference with respect to the optimized code in `RcppRoll` is large.

```{r, eval = FALSE}
(time_a/time_b)[3]
```

\bigskip

Lesson: Instead of reinventing the wheel, spend a few minutes searching the Internet to see if someone has already written a package that solves your coding problems.
 
\newpage


## Merge scanner and advertising data

Merge (join) the advertising data with the scanner data based on brand name, DMA code, and week.


## Reshape the data

Use `dcast` to reshape the data from long to wide format. The store code and week variable are the main row identifiers. Quantity, price, promotion, and adstock are the column variables.


If you inspect the data you will see many missing `adstock` values, because the adstock variable is not defined for the first `N_lags` weeks in the data. To free memory, remove all missing values from `move` (`complete.cases`).


## Time fixed effects

Create an index for each month/year combination in the data using the following code:

```{r, eval = FALSE}
move[, month_index := 12*(year(week_end) - 2011) + month(week_end)]
```


\newpage




# Data inspection

## Time-series of advertising levels

We now take a look at the advertising data. First, pick a DMA. You can easily get a list of all DMA names and codes from the `stores` table. I picked `"CHICAGO IL"`, which corresponds to `dma_code` 602. Then plot the time-series of weekly GRPs for your chosen market, separately for the own and competitor brand.

Note: I suggest you create a facet plot to display the time-series of GRPs for the two brands. Use the `facet_grid` or `facet_wrap` layer as explained in the ggplot2 guide (see "More on facetting").

## Overall advertising variation

Create a new variable **at the DMA-level**, `normalized_grp`, defined as `100*grp/mean(grp)`. This variable captures the percentage deviation of the GRP observations relative to the DMA-level mean of advertising. Plot a histogram of `normalized_grp`.

\medskip

Note: To visualize the data you should use the `scale_x_continuous` layer to set the axis `limits`. This data set is one of many examples where some extreme outliers distort the graph.

\newpage




# Advertising effect estimation

Estimate the following specifications:

1. Base specification that uses the log of `1+quantity` as output and the log of prices (own and competitor) and promotions as inputs. Control for store and month/year fixed effects.

2. Add the `adstock` (own and competitor) to specification 1.

3. Like specification 2., but not controlling for time fixed effects.

Combine the results using `etable` and comment on the results.

