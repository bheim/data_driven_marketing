---
title: "CRM and Machine Learning"
author: "Giovanni Compiani"
output:
  pdf_document:
    number_sections: yes
    toc: yes
urlcolor: blue
graphics: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE, eval = TRUE,
                      fig.width = 6, fig.height = 4.5, fig.align = "right")
```

\setlength{\parskip}{6pt}
\newpage

```{r}
library(bit64)
library(data.table)
library(glmnet)
library(ggplot2)
library(corrplot)
library(knitr)
```




# Data

The data that we use is a development sample that includes records on 250,000 randomly selected customers from the data base of a company that sells kitchenware, housewares, and specialty food products. The data include a high-dimensional set of more than 150 customer features. The random sample represents only a small percentage of the whole data base of the company.

Customers are identified by a unique `customer_id`. The targeting status of a customers as of October 10, 2017 is indicated by `mailing_indicator`. The total dollar spend (online, phone and mail order, in-store) in the subsequent 15 week period is captured by `outcome_spend`. All other variables are customer summary data based on the whole transaction history of each customer. These summary data were retrieved one week before the catalog mailing.

The customer features are largely self-explanatory. For example, `orders_online_1yr` indicates the number of orders placed during the last year. Variables such as `spend_m_1yr` indicate spending in a specific department, labeled `m`. Due to privacy reasons, what this department exactly is cannot be revealed. Similarly, no details on specific product types (e.g. `clicks_product_type_104`) can be disclosed. Also, to preserve confidentiality, some variables had to be scaled. Hence, you may see an order count such as 2.4, even though originally orders can only be 0, 1, 2, ... Please note that the scaling has no impact on the predictive power of the statistical models that you estimate. Furthermore, the restricted interpretability of some of the variables has no bearing on our analysis. Ultimately, we use the features to predict spending levels or incremental spending levels, but we do not interpret these features as causal. In particular, we cannot *manipulate* variables such as `orders_online_1yr`, which explains why the corresponding estimates have no causal interpretation.

Load the `crm_DT` data.table.

```{r}
load("Customer-Development-2017.RData")
```

\bigskip

I recommend renaming the `mailing_indicator` to make it clear that this variable represents a targeting *treatment*, $W_i$.

```{r}
setnames(crm_DT, "mailing_indicator", "W")
```

\bigskip

We split the sample into a 50% training and 50% validation sample. To ensure that we all have the same training and validation sample, use the following seed:

```{r}
set.seed(1999)
crm_DT[, training_sample := rbinom(nrow(crm_DT), 1, 0.5)]
```

\newpage




# Data inspection [15 points]

Summarize and describe some key aspects of the `outcome_spend` variable. In particular, what is the purchase incidence (i.e., what fraction of customers make a purchase), what is the distribution of dollar spending, and what is the conditional distribution of dollar spending given that a customer made a purchase?
```{r}
summary(crm_DT$outcome_spend)
purchase_incidence <- mean(crm_DT$outcome_spend > 0)
spend_distribution <- crm_DT[outcome_spend > 0, .(mean_spend = mean(outcome_spend), median_spend = median(outcome_spend))]

print(purchase_incidence)
print(spend_distribution)
```
```{r}
ggplot(crm_DT, aes(x = outcome_spend)) +
  geom_histogram(binwidth = 10, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Spending (All Customers)",
       x = "Spending ($)", y = "Number of Customers") +
  theme_minimal()
```
Comment: this has a huge outlier (or potentially a few huge outliers), should we set a max on this so the graph is somewhat legible?

```{r}
customers_with_purchase <- crm_DT[outcome_spend > 0]

ggplot(customers_with_purchase, aes(x = outcome_spend)) +
  geom_histogram(binwidth = 10, fill = "lightgreen", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Spending (Customers Who Made a Purchase)",
       x = "Spending ($)", y = "Number of Customers") +
  theme_minimal()
```
The outcome_spend variable captures the total dollar amount spent by customers within the given time frame. Analyzing it allows us to gain an insight into purchase behaviors, like the percentage of customers that ended up making a purchase as well as general spending distribution among all customers. 
From the data, we see that the mean customer spending is a mere 7.98 dollars, while the maximum stands at 1462.84 dollars. The low mean spending is a result of a purchase incidence of 6.2% (i.e. only 6.2% of customers made a purchase). Of those who did end up purchasing a product, the mean spend was 128.55 dollars, while the median purchase cost 88.95 dollars. 
Graphing the distribution of purchase amounts accentuates the fact that the vast majority of customers do not end up making a purchase. Graphing the spending from only customers who spent money, we see that the majority of customers spend between 0 and 250 dollars, with a long tail towards higher spending amounts.

\newpage




# Data pre-processing

Data sets with a large number of inputs (features) often contain highly correlated variables. The presence of such variables is not necessarily a problem if we employ an estimation method that uses regularization. However, if two variables are almost perfectly correlated then one of them captures virtually all the information contained in both variables. Also, OLS (or logistic regression) without regularization will be unfeasible with perfectly or near perfectly correlated inputs. Hence, it is helpful to eliminate some highly correlated variables from the data set.

Here is a helpful method to visualize the degree of correlation among all inputs in the data. Install the package `corrplot`. Then calculate a matrix of correlation coefficients among all inputs:

```{r}
cor_matrix = cor(crm_DT[, !c("customer_id", "W", "outcome_spend"),
                        with = FALSE])
```

Now use `corrplot` to create a pdf file that visualizes the correlation among all variables in two separate graphs. There is a huge amount of information in each graph, hence I recommend zooming in! Please see the `corrplot` documentation for a description of all the options.

```{r}
pdf("Correlation-Matrix.pdf", height = 16, width = 16)
corrplot(cor_matrix, method = "color",
         type = "lower", diag = FALSE,
         tl.cex = 0.4, tl.col = "gray10")
corrplot(cor_matrix, method = "number", number.cex = 0.25, addgrid.col = NA,
         type = "lower", diag = FALSE,
         tl.cex = 0.4, tl.col = "gray10")
dev.off()
```

Create a data table that contains the correlations for all variable pairs:

```{r}
cor_matrix[upper.tri(cor_matrix, diag = TRUE)] = NA

cor_DT = data.table(row = rep(rownames(cor_matrix), ncol(cor_matrix)),
                    col = rep(colnames(cor_matrix), each = ncol(cor_matrix)),
                    cor = as.vector(cor_matrix))
cor_DT = cor_DT[is.na(cor) == FALSE]
```

In the first statement above, we set the correlations in the upper triangle and on the diagonal of the correlation matrix to `NA`. The correlations on the diagonal are 1.0 and the correlations in the upper triangle are identical to the correlations in the lower triangle. Hence, we do not need to summarize these correlations.

Then we create a new data table, `cor_DT`, that includes all pairs of features and the respective correlation coefficient. Make sure you understand how this table is computed in the four lines of code above!

\bigskip

Now find all correlations larger than 0.95 in absolute value. Inspect these correlations, and then eliminate one of the virtually redundant variables in each highly correlated pair from the data set (to ensure that we end up with the same data, eliminate the redundant variables in the `row` column).

```{r}
large_cor_DT = cor_DT[abs(cor) > 0.95]
kable(large_cor_DT, digits = 4)
```

```{r}
crm_DT = crm_DT[, !large_cor_DT$row, with = FALSE]
```

Note that this last step eliminates from `crm_DT` all variables listed in `large_cor_DT$row`.

\newpage




# Predictive model estimation [25 points]

Use the training sample to estimate the conditional expectation of dollar spending, based on all available customer information (features). In particular, following the common approach in the industry that we have discussed in class, estimate the model only for customers who were targeted, such that $W_i = 1$. Hence, we estimate a model that predicts expected dollar spending, conditional on all customer features and conditional on being targeted. This is the same approach we have followed, e.g., in the JCPenney example in class.

Estimate and compare the following models:

1. OLS
2. LASSO
3. Elastic net

```{r}
training_DT <- crm_DT[training_sample == 1 & W == 1]

X <- model.matrix(outcome_spend ~ ., 
                 data = training_DT[, !c("customer_id", "W", "training_sample"), with = FALSE])
y <- training_DT$outcome_spend

set.seed(37105)
N_obs_training <- nrow(training_DT)
folds <- sample(1:10, N_obs_training, replace = TRUE)

# 1. OLS Model
ols_model <- lm(outcome_spend ~ ., 
                data = training_DT[, !c("customer_id", "W", "training_sample"), with = FALSE])

# 2. LASSO Model
lasso_model <- cv.glmnet(X, y, 
                        alpha = 1, 
                        nfolds = 10, 
                        foldid = folds)

# 3. Elastic Net Model
elastic_net_model <- cv.glmnet(X, y, 
                              alpha = 0.5, 
                              nfolds = 10, 
                              foldid = folds)

coef_comparison <- data.table(
  Feature = rownames(coef(ols_model)),
  OLS = as.numeric(coef(ols_model)),
  LASSO = as.numeric(coef(lasso_model, s = "lambda.min")),
  Elastic_Net = as.numeric(coef(elastic_net_model, s = "lambda.min"))
)

n_variables <- list(
  ols = length(coef(ols_model)) - 1,  # Subtract intercept
  lasso = sum(coef(lasso_model, s = "lambda.min") != 0) - 1,
  elastic_net = sum(coef(elastic_net_model, s = "lambda.min") != 0) - 1
)

lambda_min <- list(
  lasso = lasso_model$lambda.min,
  elastic_net = elastic_net_model$lambda.min
)

cat("\nNumber of variables selected by each model:\n")
cat("OLS:", n_variables$ols, "\n")
cat("LASSO:", n_variables$lasso, "\n")
cat("Elastic Net:", n_variables$elastic_net, "\n")

cat("\nOptimal lambda values:\n")
cat("LASSO lambda.min:", lambda_min$lasso, "\n")
cat("Elastic Net lambda.min:", lambda_min$elastic_net, "\n")

models <- list(
  ols = ols_model,
  lasso = lasso_model,
  elastic_net = elastic_net_model
)
```


\bigskip


**Notes**

(i) Make sure to select the correct data (training data and customers who were targeted) when estimating the models, and when creating the model matrix for the LASSO and elastic net make sure to exclude any variables that are not needed for estimation.

(ii) For replicability, provide `cv.glmnet` with the folds used for cross-validation. Set a seed, draw numbers indicating the fold ($1,2,\dots,10$) that an observation belongs to, and then supply the folds to `cv.glmnet`:

```{r indent = "     "}
set.seed(37105)

N_obs_training = nrow(training_DT)
folds = sample(1:10, N_obs_training, replace = TRUE)

fit_glmnet = cv.glmnet(X, y, foldid = folds)
```

     For comparability with my solution, please use the same seed (37105) that I set above.
    
     Note that `cv.glmnet` uses 10 folds by default. You can change the number of folds using the `nfolds` option, although this is virtually never necessary in practice.

(iii) To save on computing time, it is OK to search over a coarse set of tuning parameters $\alpha$ in the elastic net. For example, you could use a step size of 0.05.


\bigskip


Compare the estimated coefficients for OLS, the LASSO, and the elastic net. How "sparse" is the prediction problem, i.e. how many inputs are selected by the LASSO and the elastic net?

OLS, LASSO, and the elastic net each used very different numbers of inputs. OLS used a total of 146, while LASSO used 45. The elastic net used an intermediary amount, 65 inputs to be precise.
The comparison of coefficients between the models revealed a few key patterns. Firstly, OLS coefficients tend to be larger in magnitude than their LASSO and elastic net counterparts, since there is not regularization to shrink them. LASSO produced the sparsest model out of the three, with many features deemed redundant/not crucial for predicting customer spending. The coefficients of the elastic net model generally fall between the OLS and LASSO models in terms of their magnitude, reflecting possibly a more balanced approach. The model overall retained more non-zero coefficients than LASSO, but with smaller magnitudes than OLS.

Comment: should we mention that Ridge regression is used in Elastic to explain why there's an increase in #. of variables?

The fact that LASSO and Elastic Net both select significantly fewer variables than the full model while maintaining predictive power (as we'll see in the validation section) indicates that many of the original features may be redundant or not strongly predictive of customer spending.

\newpage




# Model validation [30 points]

Take the validation sample  and select only those customers who were targeted, i.e. $W_i = 1$. Using this sample, compare the observed and predicted sales outcomes.

First, compare the mean-squared error (MSE) based on the predictions of the three estimation methods.

Second, create lift tables and charts (use 20 scores/groups), plot the lifts, and compare the lift tables. I recommend **not** normalizing the lifts by the average spending in the sample, so that we can directly assess the magnitude of predicted mean spending.

Overall, how well do the models fit?

```{r}
#Step 1 - Taking the validation sample of Wi=1 from the training_sample
validation_DT <- crm_DT[training_sample == 0 & W == 1]

X_validation <- model.matrix(outcome_spend ~ ., 
                             data = validation_DT[, !c("customer_id", "W", "training_sample"), with = FALSE])
y_validation <- validation_DT$outcome_spend
```

```{r}
# Step 1 - Training Predictions
pred_ols <- predict(ols_model, newdata = validation_DT[, !c("customer_id", "W", "training_sample"), with = FALSE])
pred_lasso <- predict(lasso_model, newx = X_validation, s = "lambda.min")
pred_elastic_net <- predict(elastic_net_model, newx = X_validation, s = "lambda.min")
```

```{r}
#Step 1 - Findings of the Mean Squared Error (MSE) 
mse <- function(true, predicted) mean((true - predicted)^2)

mse_values <- data.table(
  Model = c("OLS", "LASSO", "Elastic Net"),
  MSE = c(mse(y_validation, pred_ols),
          mse(y_validation, pred_lasso),
          mse(y_validation, pred_elastic_net))
)
print(mse_values)
```

```{r}
#Step 2 - Calculating the Lift Table 
lift_table <- function(predictions, actual, n_groups = 20) {
  # Ensure numeric vectors
  predictions <- as.numeric(predictions)
  actual <- as.numeric(actual)
  
  # Create a data.table with ranks and groups
  data <- data.table(Predicted = predictions, Actual = actual)
  data[, Rank := frank(Predicted, ties.method = "first")]
  data[, Group := ceiling(Rank / (nrow(data) / n_groups))]
  
  # Compute mean predicted and actual values for each group
  lift <- data[, .(
    Predicted_Mean = mean(Predicted, na.rm = TRUE), 
    Actual_Mean = mean(Actual, na.rm = TRUE)
  ), by = Group][order(Group)]
  
  return(lift)
}
```

```{r}
# Generate lift tables
lift_ols <- lift_table(pred_ols, y_validation)
lift_lasso <- lift_table(pred_lasso, y_validation)
lift_elastic_net <- lift_table(pred_elastic_net, y_validation)

# Combine results for comparison
lift_combined <- rbind(
  cbind(lift_ols, Model = "OLS"),
  cbind(lift_lasso, Model = "LASSO"),
  cbind(lift_elastic_net, Model = "Elastic Net")
)
```

```{r}
# Plot lift chart
ggplot(lift_combined, aes(x = Group, y = Actual_Mean, color = Model)) +
  geom_line() +
  labs(
    title = "Lift Chart",
    x = "Group (Decile)",
    y = "Mean Spending",
    color = "Model"
  ) +
  theme_minimal()
```

The model with the lowest MSE (Elastic Net), which is closely followed by LASSO, fits the data better with more accuracy overall. This means that Elastic Net and LASSO effectively reduce overfitting by applying regularization, which enhances generalizability, particularly when there are many predictors or collinearities in the data. The reasoning for this is because Elastic Net Employs Ridge and LASSO Regression while LASSO reduces irrelevant coefficients to 0, which significatly reduces overfitting. 
The lift chart demonstrates similar segmentation performance across all three models, with only minor differences in decile group predictions. All models effectively identify high-value customers (Groups 15â€“20), with no clear dominance by Elastic Net or LASSO. Therefore, to this extent, all of the models properly fit the values of the original data. 

\newpage




# Traditional targeting profit prediction [20 points]

Now we work with the whole validation sample, including customers who were targeted and customers who were not targeted.

We use the preferred model that according to our previous analysis fits the data best. Using this model, we predict expected dollar spending for *all* customers in the validation sample. Consistent with standard marketing analytics practice (again think about the JCPenney example from class), we take these predictions to be indicative of what customers would spend if they were targeted, i.e.
$$\mathbb{E}(Y_{i}|\boldsymbol{x}_{i}, W_i=1).$$

and, conversely, we assume that spending is zero whenever a customer is not targeted, $\mathbb{E}(Y_{i}|\boldsymbol{x}_{i}, W_i=0) = 0$. (Here, we use $\boldsymbol{x}_{i}$ to denote the set of independent variables for customer $i$).
Given this, we predict the expected targeting profit for each customer in the validation sample. The margin and targeting cost data are:

```{r}
margin = 0.325          # 32.5%
cost   = 0.99           # 99 cents
```

\bigskip

What is the percentage of customers who should be targeted based on this analysis?

Strategy:
- Select all customers from validation sample
- Use elastic model to predict spend
- Use margin, cost to find predicted profit
- Find proportion of customers with positive profit

```{r}
validation_DT <- crm_DT[training_sample == 0]
predictions_elastic_net <- predict(elastic_net_model, newx = X_validation, s = "lambda.min")

pred_elastic_net <- predict(elastic_net_model, newx = X_validation, s = "lambda.min")

margin <- 0.325
cost <- 0.99
profit <- (pred_elastic_net * margin) - cost

positive_profit_customers <- profit[profit > 0]
proportion_to_target <- length(positive_profit_customers) / length(profit)
cat("Percent of customers to target:", proportion_to_target * 100, "\n")

```



### Part 6 Breakdown
We used elastic net since this showed the lowest MSE in our analysis in part 5. To complete this analysis, we selected all customers from the validation sample and used the elastic net model to predict their spend. We then used the margin and cost data to predict corresponding profit and then found the proportion of customers will a positive profit. Overall, we found that 59.9% of customers should be targeted based on this analysis as we predict 59.9% of customers selected to generate a positive profit through targeting (the model also tells us which customers to target, but we're not conducting that analysis here).

\newpage



# Incrementality [10 points]

Under what conditions is the analysis in the last section valid? In particular, what could invalidate the assumption that spending is zero whenever a customer is not targeted, i.e. $\mathbb{E}(Y_{i}|\boldsymbol{x}_{i}, W_i=0) = 0$, in the context of our data? Discuss.

A key assumption in our part 6 analysis is that if we don't target a customer, their spend will be 0. This does not hold if a customer would make the purchase whether or not we target them or not. Targeting is sending the catalog and there are other ways than mail/phone order to make purchases. For example, you can buy the products online or in the stores. It's possible that purchases made in those domains have little causal impact from targeting. This goes back to a similar problem from class we experienced with identifying causal forces. The information that would be most useful to us is about the incremental spend we would yield from each customer if we target them. Then, we would simply target those customers where incremental spend > targeting spend.

To get this data, we would have to conduct a randomized experiment on our customers - randoly targeting or not targeting them. This would create a counterfactual. Then, we could look at the difference in spending between the two groups and determine the incremental effect of advertising. We could break this down further by introducing covariates such as race, age, past-spending, etc. to get a better sense for how the incremental spend changes based on the characteristics of the customer. This would be a robust analysis for finding the most cost-efficient way to target customers - alas, it would also involve potentially high costs or lost revenue in the short-term due to the randomized targeting strategy.

With the data we have, we can't simply subtract the average spend on non-targeted customers in this example as the groups weren't randomized.


