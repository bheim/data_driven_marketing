---
title: "Base Pricing Analysis and Price Elasticity Estimation"
author: "Ben Heim, Marcell Milo-Sidlo, Pierre Chan, Julia Luo, Alicia Soosai"
output:
  pdf_document:
    number_sections: yes
    toc: yes
header-includes: \usepackage{booktabs}
graphics: yes
urlcolor: blue
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE, eval = TRUE,
                      fig.width = 4.5, fig.height = 3, fig.align = "right")
```

\setlength{\parskip}{6pt}
\newpage




# Overview

The goal is to conduct a base pricing analysis. We estimate brand-level demand using scanner data, and then we make profitability predictions corresponding to specific base price changes. We estimate log-linear demand models that use (log) prices and promotions as inputs, and predict log quantities, `log(1+Q)`. The models predict the demand for a focal brand, and we control for (log) prices and promotions of three competitors. Obviously, this approach generalizes to an arbitrarily large number of competing products as long as the sample size is large enough.

Our focus is on the two top brands in the liquid laundry detergent category, *Tide* and *Gain*. Both are Procter & Gamble brands. The two closest competitors are *Arm & Hammer* and *Purex*.



# Packages

Make sure to install two packages that we have not used before: fixest and knitr.

```{r}
library(bit64)
library(data.table)
library(fixest)
library(knitr)
library(ggplot2)
```

\newpage




# Data overview

The data are located in this folder:

```{r}
data_folder = "data"
```

\bigskip

The data source is an extract from the Nielsen RMS retail scanner data set. The data set captures weekly price and quantity data for all products (UPC's) sold in the stores of a large number of U.S. retail chains. The Kilts data do not include all retailers (for example, Walmart is not part of the data), and the identity of the retailers is not revealed. However, we know if two stores belong to the same retail chain.


## Brand data

The data.table `brands` in `Brands.RData` includes brand information for the top five brands in three categories (product modules):

```{}
1036   FRUIT JUICE - LEMON/LIME
1040   FRUIT JUICE - ORANGE - OTHER CONTAINER
7012   DETERGENTS - HEAVY DUTY - LIQUID
```

The data include the brand code, brand description, and total revenue calculated across all observations. The top five brands were selected based on total brand revenue.

We will focus on the liquid laundry detergent category with corresponding `product_module_code` 7012. 


## Store data

Inspect the table `stores` in the file `Stores.RData`. The variable `store_code_uc` identifies each retail stores. For some (but not all) stores we know the corresponding `retailer_code` that identifies the chain (banner) that the store belongs to. The data include the Scantrack (SMM) market code and the Scantrack market description. Scantrack markets correspond to large metropolitan market areas such as *Chicago* or *Raleigh-Durham* (see the data manual for a map of the Scantrack markets). The three-digit ZIP code of each store is also included.


## Movement data

The movement data (`move`) are in files of the form `brand_move_<module code>.RData`. The data are at the brand/store/week level and include prices and quantities (`units`). The data are aggregates of all UPC's that share the same brand name. Brand prices are measured as the weighted average over all store/week UPC prices in equivalent units, and quantities represent total product volume measured in equivalent units such as ounces. In the liquid laundry detergent category (module 7012), prices represent dollars per ounce and units are total product volume in ounces per store/week. The aggregation weights are based on total store-level UPC revenue across all weeks, and hence the aggregation weights are constant within each store. The movement data also include a promotion indicator (`promo_dummy`), a logical `TRUE/FALSE` variable.

The `week_end` variable date is the last day of a Nielsen week, which always starts on a Sunday and ends on a Saturday. Note that prices may change during the period, and hence even the UPC-level price may be an average over more than one posted price. The sample includes data for the 2010-2013 period.

Please consult the official Kilts Center Retail Scanner Dataset Manual for all details.

\newpage




# Prepare the data for the demand analysis

We first load the brand and store data.

```{r}
load(paste0(data_folder, "/Brands.RData"))
load(paste0(data_folder, "/Stores.RData"))
```


## Select the category and brands

*Choose the laundry detergent category (module) and select the corresponding brand-level meta data from the data table `brands`. Then sort (order) the brand data corresponding to total brand revenue, and select the **top four brands** (ranked by revenue).

```{r}
selected_module = 7012                 # Laundry detergent
laundry = brands[product_module_code == selected_module]
laundry_sorted = laundry[order(-revenue)][1:4]
laundry_sorted
```

\medskip

*Let's assign each brand a new name using a new variable, `brand_name`, and give the four brands simple names such as `Tide`, `Gain`, `ArmHammer`, and `Purex`. These simplified brand names will make our code and the estimation output more readable.* More specifically, create a new data containing the four selected brands and add to it the `brand_name` variable.

Note that we will add the brand names to the quantity, price, and promotion variables. In R, `price_ArmHammer` (as well as `price_Arm_Hammer`) are legal variable names, but `price_Arm&Hammer` and `price_Arm & Hammer` are not, and hence I do not suggest the brand names `Arm&Hammer` or `Arm & Hammer`.
```{r}
selected_brands = data.frame(
  product_id = c(653791, 557775, 507562, 623280),
  brand_name = c('Tide', 'Gain', 'ArmHammer', 'Purex')
)
print(selected_brands)
```

## Prepare the movement data

*Load the movement data, and---for better readability---change the variable names from `units` to `quantity` and from `promo_dummy` to `promotion` (you can use the `setnames` command for this). Change the data type of the `promotion` variable from `logical` to `numeric` using the `as.numeric` function. Finally, merge the new `brand_name` variable with the movement table (more precisely, perform an inner join, i.e. retain all observations that are present in both the parent and child data sets).
```{r}
load("data/brand_move_7012.RData")
library(data.table) 
setnames(move, old = "promo_dummy", new = "promotion", skip_absent=TRUE)
setnames(move, old = "units", new = "quantity", skip_absent=TRUE)
setnames(move, old = "brand_code_uc", new = "product_id", skip_absent=TRUE)
move$promotion <- as.numeric(move$promotion)

move_with_brands <- merge(move, selected_brands, by = "product_id", all = FALSE)
head(move_with_brands)
```
```{r}
final_data = merge(move, selected_brands, by = "product_id", all = FALSE)
print(final_data)
```

## Remove outliers

Most data contain some "flaws" or outliers. Here is an easy way of removing such outliers:

First, we create a function that flags all observations in a vector `x`, for example a price series, as outliers if the ratio between a value and the median value among all `x` observations is below or above a threshold.

```{r}
isOutlier <- function(x, threshold_bottom, threshold_top) {
   is_outlier = rep(FALSE, times = length(x))
   median_x   = median(x, na.rm = TRUE)
   is_outlier[x/median_x < threshold_bottom | x/median_x > threshold_top] = TRUE
   return(is_outlier)
}
```

*Now run this function on the price data, separately for each brand and store. Then tabulate the number of outliers, and remove the corresponding observations from the data set.*
I recommend to use a lower threshold (`threshold_bottom`) value of 0.35 and an upper threshold (`threshold_top`) of 2.5.

```{r}
threshold_bottom <- 0.35  
threshold_top <- 2.5   

final_data[, outlier_flag := isOutlier(price, threshold_bottom, threshold_top), 
           by = .(store_code_uc, brand_name)]

sum(final_data$outlier_flag)

final_data = final_data[outlier_flag == FALSE]

# removing flag
final_data[, outlier_flag := NULL]

final_data

```

## Reshape the movement data from long to wide format

To prepare the data for the regression analysis, we need to **reshape the data from long to wide format** using **`dcast`**.

All the details on casting and the reverse operation (melting from wide to long format using `melt`) are explained in the data.table html vignettes:

<https://rdatatable.gitlab.io/data.table/articles/datatable-reshape.html>

Let's be specific about the structure of the data that we need to use to estimate a demand model. We would like to obtain a table with observations, characterized by a combination of store id (`store_code_uc`) and week (`week_end`) in rows, and information on quantities, prices, and promotions in columns. Quantities, prices, and promotions are brand-specific.

```{r}
final_data = final_data[, .(store_code_uc, week_end, brand_name, quantity, price, promotion)]

final_data <- dcast(
  final_data, 
  store_code_uc + week_end ~ brand_name, 
  value.var = c("quantity", "price", "promotion")
)

```

## Merge store information with the movement data

*Now merge the movement data with the store meta data, in particular with the retailer code, the Scantrack (SMM) market code, and the Scantrack market description. But only with the store meta data where we have a valid retailer code. Hence, we need to remove store data if the retailer code is missing (`NA`). Use the `is.na` function to check if `retailer_code` is `NA` or not.*
```{r}
load("data/Stores.RData")
ls() 
print(stores)   
```
```{r}
valid_store_meta <- stores[!is.na(stores$retailer_code)]
final_data_with_store <- merge(final_data, valid_store_meta, by = "store_code_uc", all = FALSE)
head(final_data_with_store)
```

## Create time variables or trends

*A time trend records the progress of time. For example, a time trend at the week-level may equal 1 in the first week in the data, 2 in the second week, etc., whereas a trend at the month-level may equal 1 in the first month, 2 in the second month, etc.*

*I suggest you create a monthly time trend. Use the functions `year` and `month` to extract the year and month components of the week (`week_end`) variable in the movement data (alternatively, you could use the `week` function if you wanted to create a time trend at the week-level). Then, use the following code to create the monthly trend:*
```{r}
library(lubridate)
final_data_with_store$week_end <- as.Date(final_data_with_store$week_end)
final_data_with_store[, year := year(week_end)]
final_data_with_store[, month := month(week_end)]
final_data_with_store[, month_trend := 12*(year - min(year)) + month]
head(final_data_with_store[, .(week_end, year, month, month_trend)])
```

## Remove missing values

Finally, *retain only complete cases*, i.e. rows without missing values:

```{r}
final_data_with_store = final_data_with_store[complete.cases(final_data_with_store)]
```

\newpage

# Data inspection

## Observations and geographic coverage

*First, document the number of observations and the number of unique stores in the data.*

*Second, we assesss if the included stores have broad geographic coverage. We hence create a summary table that records the number of observations for each separate Scantrack market:*

```{r}
market_coverage = final_data_with_store[, .(n_obs = .N), by = SMM_description]
```

Note the use of the data.table internal `.N`: `.N` is the number of observations, either in the whole data table, or---as in this case---the number of observations within each group defined by the `by =` statement.

\medskip

A convenient way to print a table is provided by the **`kable`** function that is included in the `knitr` package. Please consult the documentation for `kable` to see all options. Particularly useful are the options `col.names`, which is used below, and `digits`, which allows you to set the number of digits to the right of the decimal point.

*Now use `kable` to document the number of observations within each Scantrack market.*

```{r}
kable(market_coverage, col.names = c("Scantrack market", "No. obs."))
```


## Price variation

Before estimating the demand models we would like to understand the degree of price variation in the data. Comment on why this is important for a regression analysis such as demand estimation!

We will predict demand for Tide and Gain. For each of these two brands separately, we would like to visualize the overall degree of price variation across observations, and also the variation in relative prices with respect to the competing brands.

- *To visualize the (own) price variation, normalize the prices of Tide and Gain by dividing by the average of these prices, and show the histogram of normalized prices.*
```{r}
tide_average = final_data_with_store[, mean(price_Tide, na.rm = TRUE)]
norm_tide = final_data_with_store[, price_Tide / tide_average]

df_tide = data.frame(norm_tide)

x_limits = quantile(df_tide$norm_tide, probs = c(0.00, 0.997), na.rm = TRUE)

ggplot(df_tide, aes(x = norm_tide)) + 
  geom_histogram(binwidth = 0.1, fill = "lightblue", color = "black") +
  scale_x_continuous(breaks = seq(min(df_tide$norm_tide), max(df_tide$norm_tide), by = 0.2), 
                     labels = scales::number_format(accuracy = 0.1),
                     limits = x_limits) +
  labs(title = "Histogram of Normalized Prices of Tide", 
       x = "Normalized Price", 
       y = "Frequency") +
  theme_minimal()

# Calculate gain_average and normalize prices for Gain
gain_average = final_data_with_store[, mean(price_Gain, na.rm = TRUE)]
norm_gain = final_data_with_store[, price_Gain / gain_average]

# Create a data frame for Gain's normalized prices
df_gain = data.frame(norm_gain)

# Plot histogram for normalized Gain prices
ggplot(df_gain, aes(x = norm_gain)) + 
  geom_histogram(binwidth = 0.1, fill = "lightblue", color = "black") +
  scale_x_continuous(breaks = seq(min(df_gain$norm_gain, na.rm = TRUE), max(df_gain$norm_gain, na.rm = TRUE), by = 0.2), 
                     labels = scales::number_format(accuracy = 0.1)) +
  labs(title = "Histogram of Normalized Prices of Gain", 
       x = "Normalized Price", 
       y = "Frequency") +
  theme_minimal()

```


- *To visualize relative prices, calculate the ratio of Tide and Gain prices with respect to the three competing brands, and show the histogram of relative prices.*

Note: To avoid that the scale of a graph is distorted by a few outliers, use the `limits` option in `scale_x_continuous` (see the ggplot2 introduction). This also helps to make the graphs comparable with each other.

```{r}
df_tide = final_data_with_store[, .(
  norm_tide = price_Tide / ((price_ArmHammer + price_Purex + price_Gain) / 3)
), by = week_end]

x_limits = quantile(df_tide$norm_tide, probs = c(0.00, 0.997), na.rm = TRUE)

ggplot(df_tide, aes(x = norm_tide)) + 
  geom_histogram(binwidth = 0.1, fill = "lightblue", color = "black") +
  scale_x_continuous(limits = x_limits) +
  labs(title = "Histogram of Relative Prices of Tide",
       x = "Relative Price of Tide (Tide / Avg Competitors)",
       y = "Frequency") +
  theme_minimal()

df_gain = final_data_with_store[, .(
  norm_ggin = price_Gain / ((price_ArmHammer + price_Purex + price_Tide) / 3)
), by = week_end]

ggplot(df_gain, aes(x = norm_gain)) + 
  geom_histogram(binwidth = 0.1, fill = "lightblue", color = "black") +
  labs(title = "Histogram of Relative Prices of Gain",
       x = "Relative Price of Gain (Gain / Avg Competitors)",
       y = "Frequency") +
  theme_minimal()


```


## Summary of data inspection

*Discuss the data description, including sample size, geographic coverage, and the results on own and relative price variation.*
From having visualized the provided data, we are able to determine that the largest single market in terms of number of observations, is Los Angeles, with 127 244 unique instances. This is followed by Chicago and Miami, with 88 257 and 56 504 observations respectively.

In terms of own price variation over time, we can see two very different graphs for Tide and Gain products. The graph of the normalized price variation of Tide tells us that for the majority of the time, the price of the product is (mostly) evenly distributed between 70% and 110% of the average price, with most instances of the product being priced between 100-110% of the average price. 
This stands in contrast to Gain products, who display a bimodal distribution, with the majority of prices being between 75%-95%, and 115%-125% of the average price. The most common price was around 80% of the average price.

Controlling for the prices of three competitors, Tide product prices were close to normally distributed around 135-140% of their competitors’ prices. In contrast, Gain displayed a bimodal distribution with peaks centered at the 80% and 120% of competitors’ prices.

\newpage




# Estimation

Now we are ready to estimate demand models for Tide and Gain.

We want to estimate a sequence of models with an increasing number of controls and compare the stability of the key results across these models. In all models the output is `log(1+quantity_<brand name>)`.

\bigskip

To keep things simple, we will initially estimate demand for Tide only.

Let's start with the following models:

1. log of own price as only input
2. Add store fixed effects
3. Add a time trend---maybe linear, or a polynomial with higher-order terms
4. Instead of a time trend add fixed effects for each month (more precisely: for each year/month combination)

*Estimate the models using the `feols` function from the fixest package (consult the corresponding fixest guide included among the R learning resources on Canvas). Store the regression outputs in some appropriately named variables (objects).*

```{r}
head(final_data_with_store)
```

```{r}
fit_base <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide),
  data = final_data_with_store
)

fit_store_FE <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) | store_code_uc,
  data = final_data_with_store
)

fit_trend <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + month | store_code_uc,
  data = final_data_with_store
)

fit_month_FE <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) | store_code_uc + year + month,
  data = final_data_with_store
)

fit_promo_Tide <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + promotion_Tide | store_code_uc + year + month,
  data = final_data_with_store
)

etable(fit_base, fit_store_FE, fit_trend, fit_month_FE, fit_promo_Tide)
```

\bigskip

**Hint**: Recall that it is perfectly legitimate in R to write model formulas such as

```{}
log(1+quantity_<brand name>) ~ log(price_<brand name>)
```

Hence, there is no need to create new variables such as the logarithm of own price, etc., before estimating a demand model.

\bigskip

You can display the regression coefficients using the `summary` function. As a much more elegant solution, however, I recommend using the `etable` function in the `fixest` package, which produces nicely formatted output. 

Please **consult the fixest guide on how to use `etable`**, and **go through the** ***Checklist for creating LaTeX tables using `etable`***!

Here is an example (note that the `fit` objects are the regression outputs---adjust the names if necessary):

```{r, results = "asis"}
#etable(fit_base, fit_store_FE, fit_trend, fit_month_FE,
       #tex = TRUE,
       #fitstat = c("n", "r2"), signif.code = NA,
       #cluster = c("store_code_uc", "month_trend"))
```

Note the option `cluster = c("store_code_uc", "month_trend")`, which tells `etable` to show standard errors that are clustered at the store and month level. These clustered standard errors will be larger and more accurate than regular standard errors because they reflect that the error terms in the regression are likely correlated at the store and month level.

\bigskip

Before moving on, you may want to remove the regression output objects that are no longer used, because they take up much space in memory:

```{r}
rm(fit_base, fit_store_FE, fit_trend)
```


## Controlling for competitor prices

*Now add the competitor prices to the demand model.*

```{r}
library(fixest)

fit_base_competitor <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain),
  data = final_data_with_store
)

fit_store_FE_competitor <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) | store_code_uc,
  data = final_data_with_store
)

fit_trend_competitor <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + month | store_code_uc,
  data = final_data_with_store
)

fit_month_FE_competitor <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) | store_code_uc + year + month,
  data = final_data_with_store
)

etable(fit_base_competitor, fit_store_FE_competitor, fit_trend_competitor, fit_month_FE_competitor)
```


## Controlling for promotions

*Now add the promotions dummies, first just for Tide, then for all brands. Compare the results. Did controlling for promotions change the own price elasticity estimate in an expected manner?*

```{r}
#Tide

fit_base_competitor_promo <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + promotion_Tide,
  data = final_data_with_store
)

fit_store_FE_competitor_promo <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + promotion_Tide | store_code_uc,
  data = final_data_with_store
)

fit_trend_competitor_promo <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + month + promotion_Tide | store_code_uc,
  data = final_data_with_store
)

fit_month_FE_competitor_promo <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + promotion_Tide | store_code_uc + year + month,
  data = final_data_with_store
)

etable(fit_base_competitor_promo, fit_store_FE_competitor_promo, fit_trend_competitor_promo, fit_month_FE_competitor_promo)
```
When comparing the models without promotions against the ones with promotions, the latter has less own price elasticity: 3.961 (promotions) vs 5.659 (no promotions). Without controlling for promotions, their effect is mistakenly attributed to price, which inflates the price elasticity. When promotions are included, the model correctly attributes the demand increase to them, resulting in a more accurate, lower price elasticity. This reflects the fact that consumers are less sensitive to price changes alone than the model without promotions suggested, as promotions are often more effective at boosting demand than small price reductions.

```{r}
# All Brands

fit_base_competitor_promo2 <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + promotion_Tide + promotion_Gain + promotion_ArmHammer + promotion_Purex, data = final_data_with_store
)

fit_store_FE_competitor_promo2 <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + promotion_Tide + promotion_Gain + promotion_ArmHammer + promotion_Purex | store_code_uc, data = final_data_with_store
)

fit_trend_competitor_promo2 <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + month + promotion_Tide + promotion_Gain + promotion_ArmHammer + promotion_Purex | store_code_uc, data = final_data_with_store
)

fit_month_FE_competitor_promo2 <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + promotion_Tide + promotion_Gain + promotion_ArmHammer + promotion_Purex | store_code_uc + year + month, data = final_data_with_store
)

etable(fit_base_competitor_promo2, fit_store_FE_competitor_promo2, fit_trend_competitor_promo2, fit_month_FE_competitor_promo2)

```

With the exception of the fit_base model, all other models are generally the same compared to the models with the single promotion. 

\bigskip

*Summarize and comment on the estimation results. Was it necessary to control for store fixed effects, time trends/fixed effects, as well as competitor prices and promotions? What do we learn from the magnitudes of the own and cross-price elasticities?*

In the base model, Tide’s own-price elasticity is quite large, with a coefficient of -7.47, indicating that a 1% increase in Tide's price would result in a 7.47% decrease in demand. However, after including store fixed effects, the magnitude of this elasticity decreases to -5.61, suggesting that the initial model without controls overestimated Tide's price sensitivity by failing to account for unobserved differences across stores. The R-squared value also jumps from 0.496 in the base model to 0.846, which demonstrates the significant improvement in model fit when store-specific factors are considered.

Time trends and month fixed effects did not have much of an impact on the overall results. After introducing a month trend, the price elasticity remains at -5.61, and the inclusion of fixed effects for each year and month leads to a slight decrease in price elasticity to -5.65. The R-squared improves marginally to 0.851, indicating that time trends capture some of the seasonality in Tide's demand, but store fixed effects remain the most important control.

Lastly, the introduction of Tide’s promotional activities further improves the model’s explanatory power, with the R-squared rising to 0.857. The own-price elasticity decreases further to -4.08, indicating that promotions significantly mitigate the negative impact of price increases. Promotions for Tide have a strong and positive effect on sales, increasing demand by approximately 36.65%. This confirms that promotions are a major driver of Tide's demand and should be accounted for when estimating price sensitivity.

\bigskip

We will use the final model including all variables (I called it `fit_promo_comp`) as our preferred model. To make this final model distinguishable from the regression output for Gain we rename it:

```{r}
fit_promo_comp <- feols(
  log(1 + quantity_Tide) ~ log(price_Tide) + log(price_Gain) + promotion_Tide + promotion_Gain + promotion_ArmHammer + promotion_Purex | store_code_uc + year + month, data = final_data_with_store
)
```

```{r}
fit_Tide = fit_promo_comp
```



## Demand model for Gain

*Now repeat the steps to estimate demand for Gain.*

```{r}
fit_base <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain),
  data = final_data_with_store
)

fit_store_FE <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) | store_code_uc,
  data = final_data_with_store
)

fit_trend <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + month | store_code_uc,
  data = final_data_with_store
)

fit_month_FE <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) | store_code_uc + year + month,
  data = final_data_with_store
)

fit_promo_Gain <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + promotion_Gain | store_code_uc + year + month,
  data = final_data_with_store
)

etable(fit_base, fit_store_FE, fit_trend, fit_month_FE, fit_promo_Gain)
```
```{r, controlling for competitor prices}
library(fixest)

fit_base_competitor <- feols(
  log(1 + quantity_Gain) ~ log(price_Tide) + log(price_Gain),
  data = final_data_with_store
)

fit_store_FE_competitor <- feols(
  log(1 + quantity_Gain) ~ log(price_Tide) + log(price_Gain) | store_code_uc,
  data = final_data_with_store
)

fit_trend_competitor <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + month | store_code_uc,
  data = final_data_with_store
)

fit_month_FE_competitor <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) | store_code_uc + year + month,
  data = final_data_with_store
)

etable(fit_base_competitor, fit_store_FE_competitor, fit_trend_competitor, fit_month_FE_competitor)
```
```{r}
#Gain

fit_base_competitor_promo <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + promotion_Gain,
  data = final_data_with_store
)

fit_store_FE_competitor_promo <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + promotion_Gain | store_code_uc,
  data = final_data_with_store
)

fit_trend_competitor_promo <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + month + promotion_Gain | store_code_uc,
  data = final_data_with_store
)

fit_month_FE_competitor_promo <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + promotion_Gain | store_code_uc + year + month,
  data = final_data_with_store
)

etable(fit_base_competitor_promo, fit_store_FE_competitor_promo, fit_trend_competitor_promo, fit_month_FE_competitor_promo)
```
```{r, promo, all brands}

fit_base_competitor_promo2 <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + promotion_Gain + promotion_Tide + promotion_ArmHammer + promotion_Purex, data = final_data_with_store
)

fit_store_FE_competitor_promo2 <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + promotion_Gain + promotion_Tide + promotion_ArmHammer + promotion_Purex | store_code_uc,  data = final_data_with_store
)

fit_trend_competitor_promo2 <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + month + promotion_Gain + promotion_Tide + promotion_ArmHammer + promotion_Purex | store_code_uc, data = final_data_with_store
)

fit_month_FE_competitor_promo2 <- feols(
  log(1 + quantity_Gain) ~ log(price_Gain) + log(price_Tide) + promotion_Gain + promotion_Tide + promotion_ArmHammer + promotion_Purex| store_code_uc + year + month,
  data = final_data_with_store
)

fit_Gain = fit_month_FE_competitor_promo2

etable(fit_base_competitor_promo, fit_store_FE_competitor_promo, fit_trend_competitor_promo, fit_month_FE_competitor_promo2)
```
*Briefly comment on the estimates, as you did before with Tide.*

The above analysis reveals a lot of the information about demand dynamics for Gain, especially the effects of its own price, competitor prices, and promotional activities. Initially, the base model suggests that Gain exhibits a strong own-price elasticity, with a 1% increase in price leading to a 9.54% decrease in demand. However, this sensitivity is notably reduced once store fixed effects are introduced, where the elasticity drops to -5.06%. This reduction indicates that the initial model overestimated price sensitivity by not accounting for the difference between stores. Additionally, the introduction of store fixed effects reveals a positive cross-price elasticity with Tide, suggesting that the two brands behave as substitutes after accounting for differences across stores. 

Promotional activities were also shown to play a substantial role in influencing demand. Interestingly, the base model showed a negative coefficient for promotion_Gain, which implied that promotions were ineffective or potentially cannibalizing future sales. However, once store-specific effects were controlled for, the promotion effect turned strongly positive, with Gain's promotion coefficient being 0.697 -- linked to an increase in sales. This shift emphasizes the importance of accounting for local market conditions when analyzing promotional effectiveness. Promotions by competing brands, such as Tide and ArmHammer, were found to have positive spillover effects on Gain's demand, indicating that competitive promotions may enhance overall consumer awareness and drive sales across brands.

Finally, the inclusion of time trends and fixed effects for months and years provided further refinement of the model, but the core findings remained consistent. Gain’s own-price elasticity remained high, though slightly reduced, and the positive cross-price elasticity with Tide continued, which confirmed the substitutability of these two brands. The strong and consistent impact of promotional activities was further reinforced, with promotions for Gain, as well as its competitors, significantly boosting demand.

...

\newpage


# Profitability analysis

The goal is to fine-tune prices jointly for Tide and Gain. We hence use the estimates of the preferred demand models and evaluate the product-line profits when we change the prices of the two brands.

\bigskip

To predict profits, let's only retain data for one year, 2013:

```{r}
move[,year := year(week_end)]
move_predict = move[year == 2013]
```

\bigskip

Although we have excellent demand data, we do not know the production costs of the brands (this is confidential information). We can infer the cost making an informed assumption on retail margins and the gross margin of the brand.  

```{r}
gross_margin  = 0.35
retail_margin = 0.18

price_Tide = move_predict$price[move_predict$product_id == 653791]
price_Gain = move_predict$price[move_predict$product_id == 557775]

cost_Tide = (1-gross_margin)*(1-retail_margin)*mean(price_Tide, na.rm = TRUE)
cost_Gain = (1-gross_margin)*(1-retail_margin)*mean(price_Gain, na.rm = TRUE)
```

As prices are measured in dollars per ounce, these marginal costs are also per ounce.

\bigskip

Now create a vector indicating the percentage price changes that we consider within an acceptable range, up to +/- 5%.

```{r}
percentage_delta = seq(-0.05, 0.05, 0.025)    # Identical to = c(-0.5, -0.025, 0.0, 0.025, 0.05)
```

\bigskip

We will consider all possible combinations of price changes for Tide and Gain. This can be easily achieved by creating a data table with the possible combinations in rows (please look at the documentation for the `rep` function):

```{r}
L = length(percentage_delta)
profit_DT = data.table(delta_Tide = rep(percentage_delta, each = L),
                       delta_Gain = rep(percentage_delta, times = L),
                       profit     = rep(0, times = L*L))
```

Inspect the resulting table. The `profit` column will allow us to store the predicted profits.

\bigskip

Now we are ready to iterate over each row in `profit_DT` and evaluate the total product-line profits of Tide and Gain for the corresponding percentage price changes. You can perform this iteration with a simple for-loop:
```{r}
avg_price_Tide <- mean(move_predict$price[move_predict$product_id == 653791], na.rm = TRUE)
avg_price_Gain <- mean(move_predict$price[move_predict$product_id == 557775], na.rm = TRUE)
for (i in 1:nrow(profit_DT)) {
   # Perform profit calculations for the price changes indicated in row i of the profit_DT table
   new_price_Tide = avg_price_Tide * (1 + profit_DT$delta_Tide[i])
   new_price_Gain = avg_price_Gain * (1 + profit_DT$delta_Gain[i])
   move_predict[product_id == 653791, price := new_price_Tide]
   move_predict[product_id == 557775, price := new_price_Gain]
   predicted_quantity_Tide <- exp(coef(fit_Tide)[1] + coef(fit_Tide)[2] * log(new_price_Tide)) - 1
   predicted_quantity_Gain <- exp(coef(fit_Gain)[1] + coef(fit_Gain)[2] * log(new_price_Gain)) - 1
   revenue_Tide = new_price_Tide * predicted_quantity_Tide
   revenue_Gain = new_price_Gain * predicted_quantity_Gain
   total_cost_Tide = cost_Tide * predicted_quantity_Tide
   total_cost_Gain = cost_Gain * predicted_quantity_Gain
   
   total_profit_Tide = revenue_Tide - total_cost_Tide
   total_profit_Gain = revenue_Gain - total_cost_Gain
   
   profit_DT$profit[i] <- total_profit_Tide + total_profit_Gain
}
print(profit_DT)
```

\medskip

Some hints:

- Before you start the loop, store the original price levels of Tide and Gain.
- Update the price columns in `move_predict` and then predict demand.
- Calculate total profits at the new price levels for both brands and then store the total profit from Tide and Gain in `profit_DT`.

\medskip

Show a table of profits in levels and in ratios relative to the baseline profit at current price levels, in order to assess the percent profit differences resulting from the contemplated price changes.

\bigskip

*Discuss the profitability predictions and how prices should be changed, if at all. How do you reconcile the recommended price changes with the own-price elasticity estimates?*


