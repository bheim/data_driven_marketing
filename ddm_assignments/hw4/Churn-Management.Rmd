---
title: "Churn Management"
author: "Ben Heim, Marcell Milo-Sidlo, Julia Luo, Pierre Chan, Alicia Soosai"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  word_document:
    toc: yes
urlcolor: blue
graphics: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE, 
                      fig.width = 4.5, fig.height = 3.25, fig.align = "right")
```

\setlength{\parskip}{6pt}
\newpage

```{r}
library(bit64)
library(data.table)
library(ggplot2)
library(broom)
library(knitr)
```




# Overview

Cell2Cell, a wireless telecommunications company (with its name altered for confidentiality), is working to reduce customer churn. The objective of this project is to create a model that predicts customer churn at Cell2Cell and to leverage insights from the model to design a targeted incentive strategy aimed at decreasing churn rates.

In the assignment, you will address these key issues:

1. Is customer churn at Cell2Cell predictable from the customer information that Cell2Cell maintains?

2. What factors drive customer churn? Which factors are particularly important?

3. What incentives should Cell2Cell offer to prevent customer churn?

4. What is the economic value of a proposed targeted plan to prevent churn, and how does this value differ across customer segments? Compare the economic value to an incentive with a cost of $100 and another incentive with a cost of $175. Which customers segments should receive the incentive? Does the answer depend on the success probability?

Note that, in what follows, the key steps you need to take are highlighted in *italic*.

\newpage




# Data

All data are contained in the file `Cell2Cell.RData`, which is posted on Canvas.

```{r}
load("Cell2Cell.RData")
ls()
```

\medskip

Please consult the file `Cell2Cell-Database-Documentation.xlsx` for a description of the data and some summary statistics. Note that *calibration sample* is an alternative term for *training* or *estimation* sample. 

*Report the churn rate in the calibration sample and in the validation sample and compare the two.*
```{r}
mean(cell2cell_DT[calibrat == 1, churn])
mean(cell2cell_DT[calibrat == 0, churn])
```
The mean of the calibration sample is .5 while the mean of the validation sample is .0196. The calibration sample's mean is significantly higher - over 25x as high.

You can see that the calibration sample was selected using *oversampling*. The purpose of oversampling was to obtain more precise estimates (lower standard errors) when estimating a logistic regression model. The validation sample, on the other hand, was not created using oversampling and represents the *true churn rate* in the data.

As you can see, some variables have missing values, which---as you know by now---is common and of no concern (unless the missing values indicate some *systematic* flaws or bias in how the data were constructed). Most estimation methods in R will automatically delete rows with missing values before estimating the model. However, the `predict` methods will yield `NA` values if a row in the data used for prediction contains missing values. Hence, in a situation where you don't need to keep the full data I recommend to remove any observations with missing values before you conduct the main analysis.

*Perform this data-cleaning step.*
```{r}
cell2cell_DT = cell2cell_DT[complete.cases(cell2cell_DT),]
```


\newpage




# Model estimation

*Estimate a logit model to predict the conditional churn probability.*

You can inspect the regression output using methods you already used, such as `summary`. Having said this, especially when you have a large number of inputs, it can be convenient to store the regression estimates in a table. A simple way to do this is to install the [broom package](https://cran.r-project.org/web/packages/broom/vignettes/broom.html) that has the purpose of cleaning up messy R output.

Using the `tidy` function in the `broom` package it is trivial to capture the regression output in the form of a data.table:

For `kable` to work, you need to load the `knitr` library.

```{r}
calibration_data = cell2cell_DT[calibrat == 1]

predictors = setdiff(names(calibration_data), c("churn", "customer", "calibrat"))

# Dynamically build the formula
formula = as.formula(paste("churn ~", paste(predictors, collapse = " + ")))

# Fit the logistic regression model
fit = glm(formula, data = calibration_data, family = binomial(link = "logit"))

results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
```

\newpage




# Prediction: Accounting for oversampling

The idea of oversampling is as follows. If the response rate in the data is small, there is a strong imbalance between observations with a response of $Y=1$ and a response of $Y=0$. As a consequence, estimating the model is difficult and the estimates will be imprecise, i.e. they will have large standard errors.

The solution: Create a training sample with one half of observations randomly chosen from the original data with response $Y=1$, and the other half randomly chosen from the original data with response $Y=0$. Now estimation is easier and the standard errors will be smaller.

However, when applied to logistic regression, oversampling will result in an inconsistent estimate of the intercept (constant) term, although all other estimates will be consistent. Hence, if we do not de-bias (adjust) the intercept, the predicted probabilities will be too large, reflecting the artificial response rate of $\frac{1}{2}$ in the over-sampled training data. 

In order to de-bias the scale of the predicted response (in this example: churn) in the validation sample we need to supply an *offset variable* to the logistic regression model. An offset is a known number that is added to the right-hand side of the regression when estimating the model, and adding the offset will correspondingly change the estimate of the intercept. The offset takes the form:
$$\text{offset}=\left[\log(\bar{p}_{t})-\log(1-\bar{p}_{t})\right]-\left[\log(\bar{p}_{v})-\log(1-\bar{p}_{v})\right]$$

Here, $\bar{p}_{t}$ is the average response rate in the training sample and $\bar{p}_{v}$ is the average response rate in the validation sample. Note that the offset is positive (given that $\bar{p}_t > \bar{p}_v$), so that including the offset term when estimating the model accounts for the fact that the training sample has a higher share of $Y=1$ relative to the validation sample.

Conversely, when we predict the response rate in the validation sample, we set the offset variable to 0.

Why does this work? --- Conceptually, logistic regression is a regression model for the log-odds of the response (outcome) probability,
$$\log \left(\frac{p}{1-p}\right) = \log(p) - \log(1-p) = \beta_0 + \beta_1 X_1 + \beta_2 X_1 + \dots $$

When we add the offset variable to the right hand side of the regression model the estimation algorithm will "incorporate" the offset in the intercept, $\beta_0$. The effect of setting the offset to 0 (when applying the model to the validation sample) is equivalent to subtracting the offset from the intercept. Subtracting the offset amounts to:

(i) Subtracting $\log(\bar{p}_{t})-\log(1-\bar{p}_{t})$, the log-odds of the artificial response rate in the training sample, and

(ii) Adding $\log(\bar{p}_{v})-\log(1-\bar{p}_{v})$, the log-odds in the validation sample that reflects the true log-odds in the data.

This process de-biases the predicted response, i.e. restores the correct response level in the validation sample.

Note: Never use over-sampling to create the validation sample, otherwise the offset variable approach will not work.

\bigskip


*Create an `offset_var` variable and add it to the data set. Then re-estimate the logistic regression. To tell `glm` that you want to use `offset_var`, you need to use a formula of the form:*

y ~ offset(offset_var) + <all other variables>

*Where* you place `offset()` on the right-hand side of the formula is irrelevant.

\medskip

*Before predicting the response rate in the validation sample set the offset to 0. Then, when you invoke the `predict` function, supply the data with the offset set to 0 using the `newdata` option.*

*Compare the average predicted response with the average observed response rate in the validation sample.*

```{r}
mean(calibration_data$churn)
```


```{r}

training_data = calibration_data

validation_data = cell2cell_DT[calibrat == 0]


p_t = mean(training_data$churn)  # Oversampled rate
p_v = mean(validation_data$churn)  # True rate in the validation sample


offset_value = (log(p_t) - log(1 - p_t)) - (log(p_v) - log(1 - p_v))
training_data[, offset_var := offset_value]


predictors = setdiff(names(training_data), c("churn", "customer", "calibrat", "offset_var"))


formula = as.formula(paste("churn ~ offset(offset_var) +", paste(predictors, collapse = " + ")))


fit = glm(formula, data = training_data, family = binomial(link = "logit"))


results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)


validation_data[, offset_var := 0]


predicted_response = predict(fit, newdata = validation_data, type = "response")


mean_predicted_response = mean(predicted_response)
mean_observed_response = mean(validation_data$churn)


list(mean_predicted = mean_predicted_response, mean_observed = mean_observed_response)
```

\newpage




# Predictive power: Lift

We evaluate the predictive fit of the logistic regression model using a lift table and lift chart. To develop reusable code, we develop a function that returns a lift table. The function (call it `liftTable`) will need to take the following inputs:

   - Predicted outcome or score
   - Observed outcome
   - Number of segments to be created based on the score
   
`liftTable` will return a data.table that contains:

   - An index (`score_group`) for each segment that was created based on the score
   - The average score value (predicted outcome) in the `score_group`
   - The average observed outcome in the `score_group`
   - The lift factor

\bigskip


To code the `liftTable` command, I recommend to use the `cut_number` function in the ggplot2 package. `cut_number` takes a variable `x` and creates `n` groups with an approximately equal number of observations in each group. Observations are assigned to the groups based on their ranking along the variable `x`. The format is:

```{}
cut_number(x, n = <no. of groups>)
```

\medskip

To illustrate, we draw 10,000 random numbers from a uniform distribution on $[0,5]$. `cut_number` assigns each number to one of five (because we set `n = 5`) groups.

```{r}
set.seed(123)
DT = data.table(x = runif(10000, min = 0, max = 5))
DT[, group    := cut_number(x, n = 5)]
DT[, group_no := as.integer(group)]
```

```{r}
head(DT)
table(DT$group)
```

\medskip

As expected, because `x` is uniformly distributed on $[0,5]$, the five groups created by `cut_number` correspond almost exactly to a $[k,k+1]$ interval ($k=0,1,\dots,4$), and each of these intervals contains exactly 20 percent of all observations based on the rank of the `x` values. The `group` variable that we created is a factor that we converted to an integer score.

\bigskip


*Calculate a lift table for 20 segments. Inspect the lift table. Then provide two charts. First, plot the `score_group` segments on the x-axis versus the observed churn rate on the y-axis. Second, plot the segments versus the lift factor, and add a horizontal line at $y=100$. How to do this in ggplot2 is explained in the ggplot2 guide (look for the `yintercept` option).*

```{r}
liftTable <- function(predicted, observed, n_segments = 20) {
  DT <- data.table(predicted = predicted, observed = observed)
  DT[, score_group := cut_number(predicted, n = n_segments)]
  DT[, score_group_no := as.integer(score_group)]
  
  lift_DT <- DT[, .(
    avg_score = mean(predicted),
    avg_observed = mean(observed),
    lift_factor = mean(observed) / mean(DT$observed)
  ), by = score_group_no]
  
  lift_DT
}
```

```{r}
lift_DT <- liftTable(predicted = predicted_response, observed = validation_data$churn, n_segments = 20)
setorder(lift_DT, score_group_no)
kable(lift_DT, digits = 3)
```

```{r}
ggplot(lift_DT, aes(x = score_group_no, y = avg_observed)) +
  geom_line() +
  geom_point() +
  labs(
    x = "Score Group",
    y = "Observed Churn Rate",
    title = "Observed Churn Rate by Score Group"
  )
```
```{r}
ggplot(lift_DT, aes(x = score_group_no, y = lift_factor * 100)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 100, linetype = "dashed", color = "red") +
  labs(
    x = "Score Group",
    y = "Lift Factor (%)",
    title = "Lift Factor by Score Group"
  )
```

\newpage


# Why do customers churn? --- Effect sizes

We would like to understand *why* customers churn, which can help us to propose incentives to prevent customer churn

*To this end, construct a table that contains comparable effect sizes (changes in the churn probability) for all independent variables, as we discussed in class.*

Here are a few more details on the steps needed to create this table:

1. Because logistic regression coefficients are not directly interpretable, we estimate a linear probability model of customer churn. In a linear probability model we regress the $Y=0,1$ output on all the customer features. The estimated coefficients can be interpreted as differences in $\Pr\{Y=1 | X_1, X_2, \dots\}$ for a one-unit difference in one of the features, $X_k$. Note: **The *offset variable* should not be included in the linear probability model as it is specific to logistic regression.**
2. Note that our analysis is based on a *comparison* of the effect sizes of the different variables. However, because the variables have different scales, the effect sizes are not directly comparable. For example, `revenue` (mean monthly revenue) and `mou` (mean monthly minutes use) have different means and standard deviations, and hence the effects of increasing `revenue` and `mou` by one unit on the churn probabilities are not comparable without taking the scale differences into account.
3. To solve this problem we **standardize** the independent variables in the data. To standardize, we divide the values of each independent variable by its standard deviation, except if the variable is a 0/1 dummy. Once standardized, all variables except the dummies will have a standard deviation of 1, and a one unit difference corresponds to a one standard deviation difference in the original, non-standardized variable.
Here's a function, `standardize_columns`, that takes a column `x` as input and returns the standardized values of the column:
```{r}
standardize_columns <- function(x) {
   
   # Check if the column is a dummy variable
   elements = unique(x)
   if (length(elements) == 2L & all(elements %in% c(0L,1L))) {
      is_dummy = TRUE
   } else {
      is_dummy = FALSE
   }
   
   # If not a dummy, divide values in x by its standard deviation
   if (is_dummy == FALSE) x = x/sd(x, na.rm = TRUE)
   
   return(x)
}
```
The first part of the function checks that the input `x` has exactly two elements and that these elements are the integers 0 and 1. Note that in R, numbers are represented as floating point numbers by default. However, adding `L` after the numbers tells R to represent the number as an integer.
```{r}
class(1)
class(1L)
```
In order to standardize all independent variables in the training data, you can use:
```{r}
DT_lin_prob = cell2cell_DT[calibrat == 1]

# Create a vector that contains the names of all inputs (covariates)
all_columns   = names(DT_lin_prob)
input_columns = all_columns[-c(1:3, length(all_columns))]

# Standardize all input columns
DT_lin_prob[, (input_columns) := lapply(.SD, standardize_columns), .SDcols = input_columns]
```
4. In order to create a table that captures the linear probability model estimates, use the `tidy` function. Add a column, e.g. `effect_size`, that scales the estimates by the factor
$$100 \cdot \frac{\bar{p}_{v}}{\bar{p}_{t}}$$
This scales the effect sizes to the correct magnitude of the churn probabilities in the validation sample and puts the effects on a 0-100% scale.
Sort the variables according to the magnitude of the effect sizes, and print the results table using `kable`.

```{r}
#Q4
# Fit Linear Probability Model
predictors = setdiff(names(DT_lin_prob), c("churn", "customer", "calibrat"))
formula = as.formula(paste("churn ~", paste(predictors, collapse = " + ")))

# Fit the logistic regression model
fit = glm(formula, data = DT_lin_prob, family = binomial(link = "logit"))

# 3. Calculate effect sizes
# Extract coefficients using tidy() from the broom package
results_DT = as.data.table(tidy(fit))

# Calculate effect size using scaling formula
p_t = mean(training_data$churn)  # Churn rate in the training data
p_v = mean(validation_data$churn)  # Churn rate in the validation data
results_DT[, effect_size := 100 * estimate * (p_v / p_t)]

# Sort results by effect size and display the table
results_DT = results_DT[order(abs(effect_size), decreasing = TRUE)]
kable(results_DT, digits = 5)
```
5. Inspect the  results. Identify some factors that are strongly associated with churn. If actionable, propose an incentive that can be targeted to the customers to prevent churn.
a. Retcall has the highest association with churn with an effect_size value of ~3.06, which is not surprising given the nature of the variable being a call to the retention team themselves. Thus, for customers with high retcall values, especially high-value customers, it is critical to first identify the trigger(s) for contacting the retention team. Following this, we can offer personalized outreach with incentives like offers, loyalty rewards, and feedback loops perhaps, to mitigate churn. Another method to proactively manage churn rates is to employ predictive analytics to identify at-risk customers before they reach out to the retention team, and proactively offer solutions tailored to their anticipated needs.
b. Eqpdays also exhibits a substantial impact on churn, with an effect_size value of ~1.42. This indicates that as equipment ages, it may become less efficient or more prone to issues, leading to increased customer dissatisfaction. To curb churn resulting from this, the eqpday variable could be utilized to identify customers with older equipment and target them with offers for upgrades or replacements. This can be framed as a loyalty perk or a preventive measure to ensure they continue to have a positive experience. Incentives in the form of discounted upgrades would also be feasible, for instance, by providing special financing options (for a limited time, perhaps) or customers due for an upgrade after a certain period. Communication in the form of maintenance tips or automated alerts based on the value of eqpdays could also prove useful in product life extension and reducing dissatisfaction from equipment issues.
c. The variable creditaa, reflecting a credit AA rating with an effect size of -1.39958 indicates that having a AA credit rating is a strong protective factor against customer churn. Customers with high credit ratings are likely more financially stable and may perceive less risk in continuing service or making long-term commitments. AA credit rating may also correlate with higher levels of customer satisfaction and trust in a good/service, suggesting these customers are more content or engaged with their current arrangements. Targeted incentives for this factor could include, firstly, offering better terms for upgrades or renewals to customers with higher credit ratings, reflecting the lower risk they pose in terms of payment delinquency. For customers interested in expensive services or products, it may be beneficial to provide flexible financing options that leverage their high credit rating to offer better interest rates or payment terms. In terms of expanding customer base (given the negative association of creditaa with churn), developing marketing campaigns that target customers with high credit ratings, emphasizing the stability and trustworthiness of the brand which aligns with their financial behavior, could help create a reliable and more stable customer target.

\newpage




# Economics of churn management

Next, we would like to predict the value of a proposed churn management program in order to assess the maximum amount that we would spend to prevent a customer from churning for one year.

*Perform this prediction*, under the following assumptions:

1. We consider a planning horizon of 4 years (the current year and three additional years), and an annual discount rate of 8 percent.
2. Predict the churn management value for 20 groups, but keep in mind that it is good practice to make sure the code works for an arbitrary number of groups in case we wish to modify that in the future.
Predict the program value for these 20 customer segments based on the predicted churn rate. Note that we create these segments based on the validation sample data. We predict current and future customer profits at the year-level. Hence, we also need to convert both the monthly churn rate and the revenue data to the year-level. 
3. Assume that the churn management program has a success probability `gamma` ($\gamma$) and compare the results for $\gamma=0.25$ and $\gamma=0.5$.

\medskip

Hint: It is easy to make little mistakes in the lifetime value predictions. Hence, be very clear about what your code is supposed to achieve, and check that every step is correct.

```{r}
# Goal: calculate LTV without churn program and LTV with churn program
# Step 1: Create groups based on predicted_churn ranking.
# Step 2: Create function calculates LTV without churn program (however, allow the function to take in a churn adjustment that we can plug gamma into) -> Sum for each year retention_rate^t * customer_profit divided by (1+.08)^t
# Step 3: Calculate this for all groups and store the values
# Step 4: Calculate this for all groups with churn adjustment of .25 and store the values
# Step 5: Calculate this for all groups with churn adjustment of .5 and store the values

# Key things to note: churn is monthly and so is revenue. Account for this fact.


create_n_groups <- function(predicted, observed, n_segments) {
  DT <- data.table(predicted_churn = predicted, observed_churn = observed$churn, observed_revenue = observed$revenue)
  DT[, score_group := cut_number(predicted, n = n_segments)]
  DT[, score_group_no := as.integer(score_group)]
  
  group_DT <- DT[, .(
    avg_score = mean(predicted_churn),
    avg_observed = 1 - (1 - mean(observed_churn))^12,
    avg_annual_revenue = 12 * mean(observed_revenue)
  ), by = score_group_no]
  
  return(group_DT);
}

#Create groups based on predicted churn with data on predicted churn, observed churn, and annual revenue. Churn is adjusted for yearly churn.
grouped_data = create_n_groups(predicted = predicted_response, observed = validation_data, n_segments=20)


calculate_LTV <- function(observed_churn, churn_program_success, annual_revenue, years, discount_rate) {
  # Key assumption: predicting average per person revenue while accounting for churn
  total_revenue = 0
  adjusted_churn_rate = observed_churn * (1 - churn_program_success)

  for(t in 1:years) {
    retention_rate = (1 - adjusted_churn_rate)^t
    discounted_cash_flow = (annual_revenue * retention_rate) / (1 + discount_rate)^t
    total_revenue = total_revenue + discounted_cash_flow
  }

  return(total_revenue)
}

# Assuming years = 4, discount_rate = 8%
years <- 4
discount_rate <- 0.08
success_probabilities <- c(0.25, 0.5)

# Calculate LTV for each group and each gamma
grouped_data[, LTV_no_program := calculate_LTV(avg_observed, 0, avg_annual_revenue, years, discount_rate)]
for (gamma in success_probabilities) {
  column_name <- paste("LTV_with_gamma", gamma, sep = "_")
  grouped_data[, (column_name) := calculate_LTV(avg_observed, gamma, avg_annual_revenue, years, discount_rate)]
}
```
```{r}
grouped_data = setorder(grouped_data, score_group_no)
grouped_data
```



\newpage




# Summarize your main results

*Please organize your main results along the four questions posed in the overview.*

Is customer churn at Cell2Cell predictable from the customer information that Cell2Cell maintains? 

Customer churn at Cell2Cell is predictable from the customer information that Cell2Cell maintains. This is because the logistic regression model demonstrated that churn is predictable, closely matching between the mean predicted churn rate (0.0194/1.94%) and the mean observed churn rate (0.0193/1.93%) in the validation sample. These calculations were conducted after having accounted for oversampling in Part 4 by creating an offset variable to the Logistic Regression model to de-bias the model. Furthermore, based on data from Part 5, metrics such as lift factors showed strong predictive capabilities, particularly in the top segments where lift factors exceeded 190%, indicating effective segmentation and modeling. For instance, the top score group table showed the highest lift factor, 194.5%, where the predicted churn probability was 4.8% and the observed churn rate was 3.8%, which reflects the model's ability to isolate customers most at risk of churning. 

Beyond this, the “avg_score” and “avg_observed” do not have large differences in each other, which presents convincing accuracy of the model. For example, in the middle range (score group 10), the predicted churn rate of 1.7% matches the observed churn rate of 1.7%, demonstrating the model's precision across all segments, not just at the extremes. Moreover, the score group 1 had a significantly lower observed churn rate of 0.3%, with a lift factor of 0.171, which confirms that the model effectively identifies and deprioritizes low-risk customers. The model’s ability to differentiate between high and low-risk segments enables Cell2Cell to allocate retention resources more efficiently.

What factors drive customer churn? Which factors are particularly important? 

Based on our findings in Part 6, some of the main driving factors increasing churn risk include retention calls (‘retcall’), equipment age (‘eqpdays’), and whether the account holder is a homemaker (‘occhmkr’). These three factors had the largest effect size, 3.07, 1.43, and 0.99 respectively, each driving up the risk of customers churning. Retention calls were the strongest indicators of churn, which suggests that customers that have made previous calls to the retention team are highly likely to leave. Longer ownership of equipment/products is also associated with highly increased churn, while the occupation of the customer also seems to have an effect, with homemakers churning most. This could be due to many factors, including but not limited to, a higher sensitivity to pricing, both in terms of changes in cell2cell’s product prices as well as the customer’s own financial situation. 
Many factors also decreased the churn risk of customers, with some of the most significant ones being the customer’s credit rating (‘creditaa’) and months in service (‘months’), with effect sizes of -1.40 and -0.79 respectively. Credit rating is the strongest indicator of customer stability, which suggests that financially stable customers are significantly less likely to churn, most likely due to a decrease in price sensitivity. Higher paying customers also seem to show more loyalty.

What incentives should Cell2Cell offer to prevent customer churn? 

To prevent customer churn, Cell2Cell should offer a combination of (1) Equipment Based Incentives, (2) Service Quality Improvements, and (3) Value-Added Services. 
First, regarding equipment-based incentives, Cell2Cell should provide proactive equipment upgrades to customers with devices that are old/obsolete. Part 5 highlights a strong relationship between high churn probabilities and factors such as equipment age (‘eqpdays’). Customers in the top group, with a predicted churn rate of 4.8% and observed churn rate of 3.8%, are nearly twice as likely to churn compared to the average. For instance, providing discounts or financing options for new devices as part of a loyalty program could keep customers engaged while addressing a key pain point. Additionally, targeting specific score groups identified in the lift table, such as deciles with lift factors above 1.5, would maximize the return on investment for such programs. They could also provide frequent upgrades in products to loyal customers as well as better pricing for cheaper cellular mobile plans. This would incentivise consumers to retain their interest in Cell2Cell, reducing churn and increasing customer loyalty. 
 	Additionally, another incentive would be to provide Service Quality Improvements. For example, the top group with a lift factor of 1.945 represents a priority group that Cell2Cell could target, which includes proactive outreach via emails in marketing new mobile service plans, enhancing reliable customer options for frustrated customers, and priority customer service for consumers that are exhibiting behaviors that coincide with increased churn rates. 
	Lastly, value added services play an important role in reducing customer churn. For customers in score groups 1–10, where lift factors are below 1, Cell2Cell could provide exclusive benefits like loyalty rewards, special bundles, or access to premium service tiers can strengthen their loyalty and deter potential churn. For financially stable customers (ex. those with strong credit scores), offering tailored financial incentives, such as extended warranties on equipment or personalized renewal offers could be an option.

What is the economic value of a proposed targeted plan to prevent churn, and how does this value differ across customer segments? Compare the economic value to an incentive with a cost of $100 and another incentive with a cost of $175. Which customer segments should receive the incentive? Does the answer depend on the success probability?

The churn management program shows a substantial increase in LTV for customer segments identified as high-risk. When the program achieves a 25% success rate, these segments exhibit significant improvements in LTV, suggesting that targeted interventions are particularly effective in mitigating churn where it is most prevalent. Increasing the success probability to 50% amplifies the economic benefits, increasing the value derived from specific segments by as much as 70%.
In terms of cost-benefit comparison, higher incentives of $175 are economically justified within segments exhibiting higher churn rates as they offset the greater retention value provided by reducing churn among these customers. For high-risk segments (groups with lift factors exceeding 1.5), higher incentives of $175 are economically justified. For instance, targeting customers in score group 20 ensures a positive ROI, as their high predicted churn risk is directly addressed. This higher expenditure is offset by the greater retention value, pointing to a strategic allocation of resources where they can generate the maximum return. Conversely, for moderate-risk segments (groups with lift factors between 100% and 150%), a $100 incentive proves to be more cost-effective, helping to manage the overall budget of the churn management program while still securing appreciable gains in customer loyalty.
	This all depends on the success probability. If success probabilities are higher, it’s more worthwhile to introduce those churn programs. If the success rate is 50%, then a $175 program would be worthwhile for groups for basically every group except group 1 (who has highest baseline LTV). However, if the success probability drops to 25% for the $175 program, then the incentive makes sense for groups 9-20 and not for groups 1-8. This is all to say that if the increase in LTV for a customer segment is higher than the spend to increase the LTV, then the company should invest in that program. The increase in LTV is inextricably linked to the success probability. The company should calculate expected ROI and look to maximize this value as they choose how to spend their churn management funds.
	Thus, the strategic recommendation for incentive allocation should place initial focus on customers with a high risk of churn as there is significant value to be obtained from these segments if churn is decreased. Cell2Cell should allocate higher incentives to top score groups with lift factors above 150%, where churn probabilities and potential economic losses are greatest. These segments stand to benefit the most from targeted retention strategies, ensuring a high ROI even with a larger incentive cost. This ensures that retention efforts are concentrated where they can have the most significant financial impact, optimizing resource use and maximizing program outcomes. 

The program's expansion should be contingent upon achieving and sustaining the projected success rates. This dynamic scaling strategy allows Cell2Cell to adjust its investments in churn management in response to effectiveness, reducing financial risk. Higher incentives can be reserved for segments where they are most likely to yield a positive ROI, while more modest incentives can be applied where lesser retention gains are needed. And, of course, continuous evaluation of the program’s fruition is critical to ensure timely adjustments and improvements can be made.




