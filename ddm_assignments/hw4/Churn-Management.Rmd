---
title: "Churn Management"
author: "Ben Heim, Marcell Milo-Sidlo, Julia Luo, Pierre Chan, Alicia Soosai"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  word_document:
    toc: yes
urlcolor: blue
graphics: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE, 
                      fig.width = 4.5, fig.height = 3.25, fig.align = "right")
```

\setlength{\parskip}{6pt}
\newpage

```{r}
library(bit64)
library(data.table)
library(ggplot2)
library(broom)
library(knitr)
```




# Overview

Cell2Cell, a wireless telecommunications company (with its name altered for confidentiality), is working to reduce customer churn. The objective of this project is to create a model that predicts customer churn at Cell2Cell and to leverage insights from the model to design a targeted incentive strategy aimed at decreasing churn rates.

In the assignment, you will address these key issues:

1. Is customer churn at Cell2Cell predictable from the customer information that Cell2Cell maintains?

2. What factors drive customer churn? Which factors are particularly important?

3. What incentives should Cell2Cell offer to prevent customer churn?

4. What is the economic value of a proposed targeted plan to prevent churn, and how does this value differ across customer segments? Compare the economic value to an incentive with a cost of $100 and another incentive with a cost of $175. Which customers segments should receive the incentive? Does the answer depend on the success probability?

Note that, in what follows, the key steps you need to take are highlighted in *italic*.

\newpage




# Data

All data are contained in the file `Cell2Cell.RData`, which is posted on Canvas.

```{r}
load("Cell2Cell.RData")
ls()
```

\medskip

Please consult the file `Cell2Cell-Database-Documentation.xlsx` for a description of the data and some summary statistics. Note that *calibration sample* is an alternative term for *training* or *estimation* sample. 

*Report the churn rate in the calibration sample and in the validation sample and compare the two.*
```{r}
mean(cell2cell_DT[calibrat == 1, churn])
mean(cell2cell_DT[calibrat == 0, churn])
```
The mean of the calibration sample is .5 while the mean of the validation sample is .0196. The calibration sample's mean is significantly higher - over 25x as high.

You can see that the calibration sample was selected using *oversampling*. The purpose of oversampling was to obtain more precise estimates (lower standard errors) when estimating a logistic regression model. The validation sample, on the other hand, was not created using oversampling and represents the *true churn rate* in the data.

As you can see, some variables have missing values, which---as you know by now---is common and of no concern (unless the missing values indicate some *systematic* flaws or bias in how the data were constructed). Most estimation methods in R will automatically delete rows with missing values before estimating the model. However, the `predict` methods will yield `NA` values if a row in the data used for prediction contains missing values. Hence, in a situation where you don't need to keep the full data I recommend to remove any observations with missing values before you conduct the main analysis.

*Perform this data-cleaning step.*
```{r}
cell2cell_DT = cell2cell_DT[complete.cases(cell2cell_DT),]
```


\newpage




# Model estimation

*Estimate a logit model to predict the conditional churn probability.*

You can inspect the regression output using methods you already used, such as `summary`. Having said this, especially when you have a large number of inputs, it can be convenient to store the regression estimates in a table. A simple way to do this is to install the [broom package](https://cran.r-project.org/web/packages/broom/vignettes/broom.html) that has the purpose of cleaning up messy R output.

Using the `tidy` function in the `broom` package it is trivial to capture the regression output in the form of a data.table:

For `kable` to work, you need to load the `knitr` library.

```{r}
calibration_data = cell2cell_DT[calibrat == 1]

predictors = setdiff(names(calibration_data), c("churn", "customer", "calibrat"))

# Dynamically build the formula
formula = as.formula(paste("churn ~", paste(predictors, collapse = " + ")))

# Fit the logistic regression model
fit = glm(formula, data = calibration_data, family = binomial(link = "logit"))

results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
```

\newpage




# Prediction: Accounting for oversampling

The idea of oversampling is as follows. If the response rate in the data is small, there is a strong imbalance between observations with a response of $Y=1$ and a response of $Y=0$. As a consequence, estimating the model is difficult and the estimates will be imprecise, i.e. they will have large standard errors.

The solution: Create a training sample with one half of observations randomly chosen from the original data with response $Y=1$, and the other half randomly chosen from the original data with response $Y=0$. Now estimation is easier and the standard errors will be smaller.

However, when applied to logistic regression, oversampling will result in an inconsistent estimate of the intercept (constant) term, although all other estimates will be consistent. Hence, if we do not de-bias (adjust) the intercept, the predicted probabilities will be too large, reflecting the artificial response rate of $\frac{1}{2}$ in the over-sampled training data. 

In order to de-bias the scale of the predicted response (in this example: churn) in the validation sample we need to supply an *offset variable* to the logistic regression model. An offset is a known number that is added to the right-hand side of the regression when estimating the model, and adding the offset will correspondingly change the estimate of the intercept. The offset takes the form:
$$\text{offset}=\left[\log(\bar{p}_{t})-\log(1-\bar{p}_{t})\right]-\left[\log(\bar{p}_{v})-\log(1-\bar{p}_{v})\right]$$

Here, $\bar{p}_{t}$ is the average response rate in the training sample and $\bar{p}_{v}$ is the average response rate in the validation sample. Note that the offset is positive (given that $\bar{p}_t > \bar{p}_v$), so that including the offset term when estimating the model accounts for the fact that the training sample has a higher share of $Y=1$ relative to the validation sample.

Conversely, when we predict the response rate in the validation sample, we set the offset variable to 0.

Why does this work? --- Conceptually, logistic regression is a regression model for the log-odds of the response (outcome) probability,
$$\log \left(\frac{p}{1-p}\right) = \log(p) - \log(1-p) = \beta_0 + \beta_1 X_1 + \beta_2 X_1 + \dots $$

When we add the offset variable to the right hand side of the regression model the estimation algorithm will "incorporate" the offset in the intercept, $\beta_0$. The effect of setting the offset to 0 (when applying the model to the validation sample) is equivalent to subtracting the offset from the intercept. Subtracting the offset amounts to:

(i) Subtracting $\log(\bar{p}_{t})-\log(1-\bar{p}_{t})$, the log-odds of the artificial response rate in the training sample, and

(ii) Adding $\log(\bar{p}_{v})-\log(1-\bar{p}_{v})$, the log-odds in the validation sample that reflects the true log-odds in the data.

This process de-biases the predicted response, i.e. restores the correct response level in the validation sample.

Note: Never use over-sampling to create the validation sample, otherwise the offset variable approach will not work.

\bigskip


*Create an `offset_var` variable and add it to the data set. Then re-estimate the logistic regression. To tell `glm` that you want to use `offset_var`, you need to use a formula of the form:*

y ~ offset(offset_var) + <all other variables>

*Where* you place `offset()` on the right-hand side of the formula is irrelevant.

\medskip

*Before predicting the response rate in the validation sample set the offset to 0. Then, when you invoke the `predict` function, supply the data with the offset set to 0 using the `newdata` option.*

*Compare the average predicted response with the average observed response rate in the validation sample.*

```{r}
mean(calibration_data$churn)
```


```{r}


data_Y1 = cell2cell_DT[churn == 1]
data_Y0 = cell2cell_DT[churn == 0]

#TODO: this is making the training data include part of the validation sample. 
set.seed(11111)  # Set seed for reproducibility
sample_size = min(nrow(data_Y1), nrow(data_Y0))
training_data = rbind(
  data_Y1[sample(.N, sample_size)],
  data_Y0[sample(.N, sample_size)]
)

training_data = calibration_data

validation_data = cell2cell_DT[calibrat == 0]


p_t = mean(training_data$churn)  # Oversampled rate
p_v = mean(validation_data$churn)  # True rate in the validation sample


offset_value = (log(p_t) - log(1 - p_t)) - (log(p_v) - log(1 - p_v))
training_data[, offset_var := offset_value]


predictors = setdiff(names(training_data), c("churn", "customer", "calibrat", "offset_var"))


formula = as.formula(paste("churn ~ offset(offset_var) +", paste(predictors, collapse = " + ")))


fit = glm(formula, data = training_data, family = binomial(link = "logit"))


results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)


validation_data[, offset_var := 0]


predicted_response = predict(fit, newdata = validation_data, type = "response")


mean_predicted_response = mean(predicted_response)
mean_observed_response = mean(validation_data$churn)


list(mean_predicted = mean_predicted_response, mean_observed = mean_observed_response)
```

\newpage




# Predictive power: Lift

We evaluate the predictive fit of the logistic regression model using a lift table and lift chart. To develop reusable code, we develop a function that returns a lift table. The function (call it `liftTable`) will need to take the following inputs:

   - Predicted outcome or score
   - Observed outcome
   - Number of segments to be created based on the score
   
`liftTable` will return a data.table that contains:

   - An index (`score_group`) for each segment that was created based on the score
   - The average score value (predicted outcome) in the `score_group`
   - The average observed outcome in the `score_group`
   - The lift factor

\bigskip


To code the `liftTable` command, I recommend to use the `cut_number` function in the ggplot2 package. `cut_number` takes a variable `x` and creates `n` groups with an approximately equal number of observations in each group. Observations are assigned to the groups based on their ranking along the variable `x`. The format is:

```{}
cut_number(x, n = <no. of groups>)
```

\medskip

To illustrate, we draw 10,000 random numbers from a uniform distribution on $[0,5]$. `cut_number` assigns each number to one of five (because we set `n = 5`) groups.

```{r}
set.seed(123)
DT = data.table(x = runif(10000, min = 0, max = 5))
DT[, group    := cut_number(x, n = 5)]
DT[, group_no := as.integer(group)]
```

```{r}
head(DT)
table(DT$group)
```

\medskip

As expected, because `x` is uniformly distributed on $[0,5]$, the five groups created by `cut_number` correspond almost exactly to a $[k,k+1]$ interval ($k=0,1,\dots,4$), and each of these intervals contains exactly 20 percent of all observations based on the rank of the `x` values. The `group` variable that we created is a factor that we converted to an integer score.

\bigskip


*Calculate a lift table for 20 segments. Inspect the lift table. Then provide two charts. First, plot the `score_group` segments on the x-axis versus the observed churn rate on the y-axis. Second, plot the segments versus the lift factor, and add a horizontal line at $y=100$. How to do this in ggplot2 is explained in the ggplot2 guide (look for the `yintercept` option).*

```{r}
liftTable <- function(predicted, observed, n_segments = 20) {
  DT <- data.table(predicted = predicted, observed = observed)
  DT[, score_group := cut_number(predicted, n = n_segments)]
  DT[, score_group_no := as.integer(score_group)]
  
  lift_DT <- DT[, .(
    avg_score = mean(predicted),
    avg_observed = mean(observed),
    lift_factor = mean(observed) / mean(DT$observed)
  ), by = score_group_no]
  
  lift_DT
}
```

```{r}
lift_DT <- liftTable(predicted = predicted_response, observed = validation_data$churn, n_segments = 20)
kable(lift_DT, digits = 3)
```

```{r}
ggplot(lift_DT, aes(x = score_group_no, y = avg_observed)) +
  geom_line() +
  geom_point() +
  labs(
    x = "Score Group",
    y = "Observed Churn Rate",
    title = "Observed Churn Rate by Score Group"
  )
```
```{r}
ggplot(lift_DT, aes(x = score_group_no, y = lift_factor * 100)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 100, linetype = "dashed", color = "red") +
  labs(
    x = "Score Group",
    y = "Lift Factor (%)",
    title = "Lift Factor by Score Group"
  )
```

\newpage


# Why do customers churn? --- Effect sizes

We would like to understand *why* customers churn, which can help us to propose incentives to prevent customer churn

*To this end, construct a table that contains comparable effect sizes (changes in the churn probability) for all independent variables, as we discussed in class.*

Here are a few more details on the steps needed to create this table:

1. Because logistic regression coefficients are not directly interpretable, we estimate a linear probability model of customer churn. In a linear probability model we regress the $Y=0,1$ output on all the customer features. The estimated coefficients can be interpreted as differences in $\Pr\{Y=1 | X_1, X_2, \dots\}$ for a one-unit difference in one of the features, $X_k$. Note: **The *offset variable* should not be included in the linear probability model as it is specific to logistic regression.**
2. Note that our analysis is based on a *comparison* of the effect sizes of the different variables. However, because the variables have different scales, the effect sizes are not directly comparable. For example, `revenue` (mean monthly revenue) and `mou` (mean monthly minutes use) have different means and standard deviations, and hence the effects of increasing `revenue` and `mou` by one unit on the churn probabilities are not comparable without taking the scale differences into account.
3. To solve this problem we **standardize** the independent variables in the data. To standardize, we divide the values of each independent variable by its standard deviation, except if the variable is a 0/1 dummy. Once standardized, all variables except the dummies will have a standard deviation of 1, and a one unit difference corresponds to a one standard deviation difference in the original, non-standardized variable.
Here's a function, `standardize_columns`, that takes a column `x` as input and returns the standardized values of the column:
```{r}
standardize_columns <- function(x) {
   
   # Check if the column is a dummy variable
   elements = unique(x)
   if (length(elements) == 2L & all(elements %in% c(0L,1L))) {
      is_dummy = TRUE
   } else {
      is_dummy = FALSE
   }
   
   # If not a dummy, divide values in x by its standard deviation
   if (is_dummy == FALSE) x = x/sd(x, na.rm = TRUE)
   
   return(x)
}
```
The first part of the function checks that the input `x` has exactly two elements and that these elements are the integers 0 and 1. Note that in R, numbers are represented as floating point numbers by default. However, adding `L` after the numbers tells R to represent the number as an integer.
```{r}
class(1)
class(1L)
```
In order to standardize all independent variables in the training data, you can use:
```{r}
DT_lin_prob = cell2cell_DT[calibrat == 1]

# Create a vector that contains the names of all inputs (covariates)
all_columns   = names(DT_lin_prob)
input_columns = all_columns[-c(1:3, length(all_columns))]

# Standardize all input columns
DT_lin_prob[, (input_columns) := lapply(.SD, standardize_columns), .SDcols = input_columns]
```
4. In order to create a table that captures the linear probability model estimates, use the `tidy` function. Add a column, e.g. `effect_size`, that scales the estimates by the factor
$$100 \cdot \frac{\bar{p}_{v}}{\bar{p}_{t}}$$
This scales the effect sizes to the correct magnitude of the churn probabilities in the validation sample and puts the effects on a 0-100% scale.
Sort the variables according to the magnitude of the effect sizes, and print the results table using `kable`.

```{r}
#Q4
# Fit Linear Probability Model
predictors = setdiff(names(DT_lin_prob), c("churn", "customer", "calibrat"))
formula = as.formula(paste("churn ~", paste(predictors, collapse = " + ")))

# Fit the logistic regression model
fit = glm(formula, data = DT_lin_prob, family = binomial(link = "logit"))

# 3. Calculate effect sizes
# Extract coefficients using tidy() from the broom package
results_DT = as.data.table(tidy(fit))

# Calculate effect size using scaling formula
p_t = mean(training_data$churn)  # Churn rate in the training data
p_v = mean(validation_data$churn)  # Churn rate in the validation data
results_DT[, effect_size := 100 * estimate * (p_v / p_t)]

# Sort results by effect size and display the table
results_DT = results_DT[order(abs(effect_size), decreasing = TRUE)]
kable(results_DT, digits = 5)
```
5. Inspect the  results. Identify some factors that are strongly associated with churn. If actionable, propose an incentive that can be targeted to the customers to prevent churn.
a. Retcall has the highest association with churn with an effect_size value of ~3.06, which is not surprising given the nature of the variable being a call to the retention team themselves. Thus, for customers with high retcall values, especially high-value customers, it is critical to first identify the trigger(s) for contacting the retention team. Following this, we can offer personalized outreach with incentives like offers, loyalty rewards, and feedback loops perhaps, to mitigate churn. Another method to proactively manage churn rates is to employ predictive analytics to identify at-risk customers before they reach out to the retention team, and proactively offer solutions tailored to their anticipated needs.
b. Eqpdays also exhibits a substantial impact on churn, with an effect_size value of ~1.42. This indicates that as equipment ages, it may become less efficient or more prone to issues, leading to increased customer dissatisfaction. To curb churn resulting from this, the eqpday variable could be utilized to identify customers with older equipment and target them with offers for upgrades or replacements. This can be framed as a loyalty perk or a preventive measure to ensure they continue to have a positive experience. Incentives in the form of discounted upgrades would also be feasible, for instance, by providing special financing options (for a limited time, perhaps) or customers due for an upgrade after a certain period. Communication in the form of maintenance tips or automated alerts based on the value of eqpdays could also prove useful in product life extension and reducing dissatisfaction from equipment issues.
c. The variable creditaa, reflecting a credit AA rating with an effect size of -1.39958 indicates that having a AA credit rating is a strong protective factor against customer churn. Customers with high credit ratings are likely more financially stable and may perceive less risk in continuing service or making long-term commitments. AA credit rating may also correlate with higher levels of customer satisfaction and trust in a good/service, suggesting these customers are more content or engaged with their current arrangements. Targeted incentives for this factor could include, firstly, offering better terms for upgrades or renewals to customers with higher credit ratings, reflecting the lower risk they pose in terms of payment delinquency. For customers interested in expensive services or products, it may be beneficial to provide flexible financing options that leverage their high credit rating to offer better interest rates or payment terms. In terms of expanding customer base (given the negative association of creditaa with churn), developing marketing campaigns that target customers with high credit ratings, emphasizing the stability and trustworthiness of the brand which aligns with their financial behavior, could help create a reliable and more stable customer target.

\newpage




# Economics of churn management

Next, we would like to predict the value of a proposed churn management program in order to assess the maximum amount that we would spend to prevent a customer from churning for one year.

*Perform this prediction*, under the following assumptions:

1. We consider a planning horizon of 4 years (the current year and three additional years), and an annual discount rate of 8 percent.
2. Predict the churn management value for 20 groups, but keep in mind that it is good practice to make sure the code works for an arbitrary number of groups in case we wish to modify that in the future.
Predict the program value for these 20 customer segments based on the predicted churn rate. Note that we create these segments based on the validation sample data. We predict current and future customer profits at the year-level. Hence, we also need to convert both the monthly churn rate and the revenue data to the year-level. 
3. Assume that the churn management program has a success probability `gamma` ($\gamma$) and compare the results for $\gamma=0.25$ and $\gamma=0.5$.

\medskip

Hint: It is easy to make little mistakes in the lifetime value predictions. Hence, be very clear about what your code is supposed to achieve, and check that every step is correct.

```{r}
# Goal: calculate LTV without churn program and LTV with churn program
# Step 1: Create groups based on predicted_churn ranking.
# Step 2: Create function calculates LTV without churn program (however, allow the function to take in a churn adjustment that we can plug gamma into) -> Sum for each year retention_rate^t * customer_profit divided by (1+.08)^t
# Step 3: Calculate this for all groups and store the values
# Step 4: Calculate this for all groups with churn adjustment of .25 and store the values
# Step 5: Calculate this for all groups with churn adjustment of .5 and store the values

# Key things to note: churn is monthly and so is revenue. Account for this fact.


create_n_groups <- function(predicted, observed, n_segments) {
  DT <- data.table(predicted_churn = predicted, observed_churn = observed$churn, observed_revenue = observed$revenue)
  DT[, score_group := cut_number(predicted, n = n_segments)]
  DT[, score_group_no := as.integer(score_group)]
  
  group_DT <- DT[, .(
    avg_score = 12 * mean(predicted_churn),
    avg_observed = 12 * mean(observed_churn),
    avg_annual_revenue = 12 * mean(observed_revenue)
  ), by = score_group_no]
  
  return(group_DT);
}

#Create groups based on predicted churn with data on predicted churn, observed churn, and annual revenue. Churn is adjusted for yearly churn.
grouped_data = create_n_groups(predicted = predicted_response, observed = validation_data, n_segments=20)

grouped_data

calculate_LTV = function(observed_churn, churn_program_success, annual_revenue, years) {
  # Key assumption: predicting average per person revenue while accounting for churn
  total_revenue = 0
  for(i in 0:years) {
    total_revenue = total_revenue + ((1 - (observed_churn * (1 - churn_program_sucess)))^i * annual_revenue / (1 + .08)^i)
  }
}



```



\newpage




# Summarize your main results

*Please organize your main results along the four questions posed in the overview.*


