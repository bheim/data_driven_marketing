results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
validation_data[, offset_var := 0]
predicted_response = predict(fit, newdata = validation_data, type = "response")
mean_predicted_response = mean(predicted_response)
mean_observed_response = mean(validation_data$churn)
list(mean_predicted = mean_predicted_response, mean_observed = mean_observed_response)
print(mean(training_data$churn))
data_Y1 = cell2cell_DT[churn == 1]
data_Y0 = cell2cell_DT[churn == 0]
#TODO: this is making the training data include part of the validation sample.
set.seed(11111)  # Set seed for reproducibility
sample_size = min(nrow(data_Y1), nrow(data_Y0))
training_data = rbind(
data_Y1[sample(.N, sample_size)],
data_Y0[sample(.N, sample_size)]
)
training_data = calibration_data
validation_data = cell2cell_DT[calibrat == 0]
p_t = mean(training_data$churn)  # Oversampled rate
p_v = mean(validation_data$churn)  # True rate in the validation sample
offset_value = (log(p_t) - log(1 - p_t)) - (log(p_v) - log(1 - p_v))
training_data[, offset_var := offset_value]
predictors = setdiff(names(training_data), c("churn", "customer", "calibrat", "offset_var"))
formula = as.formula(paste("churn ~ offset(offset_var) +", paste(predictors, collapse = " + ")))
fit = glm(formula, data = training_data, family = binomial(link = "logit"))
results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
validation_data[, offset_var := 0]
predicted_response = predict(fit, newdata = validation_data, type = "response")
mean_predicted_response = mean(predicted_response)
mean_observed_response = mean(validation_data$churn)
list(mean_predicted = mean_predicted_response, mean_observed = mean_observed_response)
print(mean(training_data$churn))
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE,
fig.width = 4.5, fig.height = 3.25, fig.align = "right")
library(bit64)
library(data.table)
library(ggplot2)
library(broom)
library(knitr)
load("Cell2Cell.RData")
ls()
mean(cell2cell_DT[calibrat == 1, churn])
mean(cell2cell_DT[calibrat == 0, churn])
cell2cell_DT = cell2cell_DT[complete.cases(cell2cell_DT),]
calibration_data = cell2cell_DT[calibrat == 1]
predictors = setdiff(names(calibration_data), c("churn", "customer", "calibrat"))
# Dynamically build the formula
formula = as.formula(paste("churn ~", paste(predictors, collapse = " + ")))
# Fit the logistic regression model
fit = glm(formula, data = calibration_data, family = binomial(link = "logit"))
results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
mean(calibration_data$churn)
data_Y1 = cell2cell_DT[churn == 1]
data_Y0 = cell2cell_DT[churn == 0]
#TODO: this is making the training data include part of the validation sample.
set.seed(11111)  # Set seed for reproducibility
sample_size = min(nrow(data_Y1), nrow(data_Y0))
training_data = rbind(
data_Y1[sample(.N, sample_size)],
data_Y0[sample(.N, sample_size)]
)
training_data = calibration_data
validation_data = cell2cell_DT[calibrat == 0]
p_t = mean(training_data$churn)  # Oversampled rate
p_v = mean(validation_data$churn)  # True rate in the validation sample
offset_value = (log(p_t) - log(1 - p_t)) - (log(p_v) - log(1 - p_v))
training_data[, offset_var := offset_value]
predictors = setdiff(names(training_data), c("churn", "customer", "calibrat", "offset_var"))
formula = as.formula(paste("churn ~ offset(offset_var) +", paste(predictors, collapse = " + ")))
fit = glm(formula, data = training_data, family = binomial(link = "logit"))
results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
validation_data[, offset_var := 0]
predicted_response = predict(fit, newdata = validation_data, type = "response")
mean_predicted_response = mean(predicted_response)
mean_observed_response = mean(validation_data$churn)
list(mean_predicted = mean_predicted_response, mean_observed = mean_observed_response)
print(mean(training_data$churn))
set.seed(123)
DT = data.table(x = runif(10000, min = 0, max = 5))
DT[, group    := cut_number(x, n = 5)]
DT[, group_no := as.integer(group)]
head(DT)
table(DT$group)
liftTable <- function(predicted, observed, n_segments = 20) {
DT <- data.table(predicted = predicted, observed = observed)
DT[, score_group := cut_number(predicted, n = n_segments)]
DT[, score_group_no := as.integer(score_group)]
lift_DT <- DT[, .(
avg_score = mean(predicted),
avg_observed = mean(observed),
lift_factor = mean(observed) / mean(DT$observed)
), by = score_group_no]
lift_DT
}
lift_DT <- liftTable(predicted = predicted_response, observed = validation_data$churn, n_segments = 20)
kable(lift_DT, digits = 3)
ggplot(lift_DT, aes(x = score_group_no, y = avg_observed)) +
geom_line() +
geom_point() +
labs(
x = "Score Group",
y = "Observed Churn Rate",
title = "Observed Churn Rate by Score Group"
)
ggplot(lift_DT, aes(x = score_group_no, y = lift_factor * 100)) +
geom_line() +
geom_point() +
geom_hline(yintercept = 100, linetype = "dashed", color = "red") +
labs(
x = "Score Group",
y = "Lift Factor (%)",
title = "Lift Factor by Score Group"
)
standardize_columns <- function(x) {
# Check if the column is a dummy variable
elements = unique(x)
if (length(elements) == 2L & all(elements %in% c(0L,1L))) {
is_dummy = TRUE
} else {
is_dummy = FALSE
}
# If not a dummy, divide values in x by its standard deviation
if (is_dummy == FALSE) x = x/sd(x, na.rm = TRUE)
return(x)
}
class(1)
class(1L)
DT_lin_prob = cell2cell_DT[calibrat == 1]
# Create a vector that contains the names of all inputs (covariates)
all_columns   = names(DT_lin_prob)
input_columns = all_columns[-c(1:3, length(all_columns))]
# Standardize all input columns
DT_lin_prob[, (input_columns) := lapply(.SD, standardize_columns), .SDcols = input_columns]
#Q4
# Fit Linear Probability Model
predictors = setdiff(names(DT_lin_prob), c("churn", "customer", "calibrat"))
formula = as.formula(paste("churn ~", paste(predictors, collapse = " + ")))
# Fit the logistic regression model
fit = glm(formula, data = DT_lin_prob, family = binomial(link = "logit"))
# 3. Calculate effect sizes
# Extract coefficients using tidy() from the broom package
results_DT = as.data.table(tidy(fit))
# Calculate effect size using scaling formula
p_t = mean(training_data$churn)  # Churn rate in the training data
p_v = mean(validation_data$churn)  # Churn rate in the validation data
results_DT[, effect_size := 100 * estimate * (p_v / p_t)]
# Sort results by effect size and display the table
results_DT = results_DT[order(abs(effect_size), decreasing = TRUE)]
kable(results_DT, digits = 5)
# Goal: calculate LTV without churn program and LTV with churn program
# Step 1: Create groups based on predicted_churn ranking.
# Step 2: Create function calculates LTV without churn program (however, allow the function to take in a churn adjustment that we can plug gamma into) -> Sum for each year retention_rate^t * customer_profit divided by (1+.08)^t
# Step 3: Calculate this for all groups and store the values
# Step 4: Calculate this for all groups with churn adjustment of .25 and store the values
# Step 5: Calculate this for all groups with churn adjustment of .5 and store the values
# Key things to note: churn is monthly and so is revenue. Account for this fact.
create_n_groups <- function(predicted, observed, n_segments) {
DT <- data.table(predicted_churn = predicted, observed_churn = observed$churn, observed_revenue = observed$revenue)
DT[, score_group := cut_number(predicted, n = n_segments)]
DT[, score_group_no := as.integer(score_group)]
group_DT <- DT[, .(
avg_score = mean(predicted_churn),
avg_observed = 1 - (1 - mean(observed_churn))^12,
avg_annual_revenue = 12 * mean(observed_revenue)
), by = score_group_no]
return(group_DT);
}
#Create groups based on predicted churn with data on predicted churn, observed churn, and annual revenue. Churn is adjusted for yearly churn.
grouped_data = create_n_groups(predicted = predicted_response, observed = validation_data, n_segments=20)
calculate_LTV <- function(observed_churn, churn_program_success, annual_revenue, years, discount_rate) {
# Key assumption: predicting average per person revenue while accounting for churn
total_revenue = 0
adjusted_churn_rate = observed_churn * (1 - churn_program_success)
for(t in 1:years) {
retention_rate = (1 - adjusted_churn_rate)^t
discounted_cash_flow = (annual_revenue * retention_rate) / (1 + discount_rate)^t
total_revenue = total_revenue + discounted_cash_flow
}
return(total_revenue)
}
# Assuming years = 4, discount_rate = 8%
years <- 4
discount_rate <- 0.08
success_probabilities <- c(0.25, 0.5)
# Calculate LTV for each group and each gamma
grouped_data[, LTV_no_program := calculate_LTV(avg_observed, 0, avg_annual_revenue, years, discount_rate)]
for (gamma in success_probabilities) {
column_name <- paste("LTV_with_gamma", gamma, sep = "_")
grouped_data[, (column_name) := calculate_LTV(avg_observed, gamma, avg_annual_revenue, years, discount_rate)]
}
grouped_data = setorder(grouped_data, score_group_no)
grouped_data
data_Y1 = cell2cell_DT[churn == 1]
data_Y0 = cell2cell_DT[churn == 0]
#TODO: this is making the training data include part of the validation sample.
set.seed(11111)  # Set seed for reproducibility
sample_size = min(nrow(data_Y1), nrow(data_Y0))
training_data = rbind(
data_Y1[sample(.N, sample_size)],
data_Y0[sample(.N, sample_size)]
)
training_data = calibration_data
validation_data = cell2cell_DT[calibrat == 0]
p_t = mean(training_data$churn)  # Oversampled rate
p_v = mean(validation_data$churn)  # True rate in the validation sample
offset_value = (log(p_t) - log(1 - p_t)) - (log(p_v) - log(1 - p_v))
training_data[, offset_var := offset_value]
predictors = setdiff(names(training_data), c("churn", "customer", "calibrat", "offset_var"))
formula = as.formula(paste("churn ~ offset(offset_var) +", paste(predictors, collapse = " + ")))
fit = glm(formula, data = training_data, family = binomial(link = "logit"))
results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
validation_data[, offset_var := 0]
predicted_response = predict(fit, newdata = validation_data, type = "response")
mean_predicted_response = mean(predicted_response)
mean_observed_response = mean(validation_data$churn)
list(mean_predicted = mean_predicted_response, mean_observed = mean_observed_response)
data_Y1 = cell2cell_DT[churn == 1]
data_Y0 = cell2cell_DT[churn == 0]
#TODO: this is making the training data include part of the validation sample.
set.seed(11111)  # Set seed for reproducibility
training_data = calibration_data
validation_data = cell2cell_DT[calibrat == 0]
p_t = mean(training_data$churn)  # Oversampled rate
p_v = mean(validation_data$churn)  # True rate in the validation sample
offset_value = (log(p_t) - log(1 - p_t)) - (log(p_v) - log(1 - p_v))
training_data[, offset_var := offset_value]
predictors = setdiff(names(training_data), c("churn", "customer", "calibrat", "offset_var"))
formula = as.formula(paste("churn ~ offset(offset_var) +", paste(predictors, collapse = " + ")))
fit = glm(formula, data = training_data, family = binomial(link = "logit"))
results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
validation_data[, offset_var := 0]
predicted_response = predict(fit, newdata = validation_data, type = "response")
mean_predicted_response = mean(predicted_response)
mean_observed_response = mean(validation_data$churn)
list(mean_predicted = mean_predicted_response, mean_observed = mean_observed_response)
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE,
fig.width = 4.5, fig.height = 3.25, fig.align = "right")
library(bit64)
library(data.table)
library(ggplot2)
library(broom)
library(knitr)
load("Cell2Cell.RData")
ls()
mean(cell2cell_DT[calibrat == 1, churn])
mean(cell2cell_DT[calibrat == 0, churn])
cell2cell_DT = cell2cell_DT[complete.cases(cell2cell_DT),]
calibration_data = cell2cell_DT[calibrat == 1]
predictors = setdiff(names(calibration_data), c("churn", "customer", "calibrat"))
# Dynamically build the formula
formula = as.formula(paste("churn ~", paste(predictors, collapse = " + ")))
# Fit the logistic regression model
fit = glm(formula, data = calibration_data, family = binomial(link = "logit"))
results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
mean(calibration_data$churn)
data_Y1 = cell2cell_DT[churn == 1]
data_Y0 = cell2cell_DT[churn == 0]
training_data = calibration_data
validation_data = cell2cell_DT[calibrat == 0]
p_t = mean(training_data$churn)  # Oversampled rate
p_v = mean(validation_data$churn)  # True rate in the validation sample
offset_value = (log(p_t) - log(1 - p_t)) - (log(p_v) - log(1 - p_v))
training_data[, offset_var := offset_value]
predictors = setdiff(names(training_data), c("churn", "customer", "calibrat", "offset_var"))
formula = as.formula(paste("churn ~ offset(offset_var) +", paste(predictors, collapse = " + ")))
fit = glm(formula, data = training_data, family = binomial(link = "logit"))
results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
validation_data[, offset_var := 0]
predicted_response = predict(fit, newdata = validation_data, type = "response")
mean_predicted_response = mean(predicted_response)
mean_observed_response = mean(validation_data$churn)
list(mean_predicted = mean_predicted_response, mean_observed = mean_observed_response)
set.seed(123)
DT = data.table(x = runif(10000, min = 0, max = 5))
DT[, group    := cut_number(x, n = 5)]
DT[, group_no := as.integer(group)]
head(DT)
table(DT$group)
liftTable <- function(predicted, observed, n_segments = 20) {
DT <- data.table(predicted = predicted, observed = observed)
DT[, score_group := cut_number(predicted, n = n_segments)]
DT[, score_group_no := as.integer(score_group)]
lift_DT <- DT[, .(
avg_score = mean(predicted),
avg_observed = mean(observed),
lift_factor = mean(observed) / mean(DT$observed)
), by = score_group_no]
lift_DT
}
lift_DT <- liftTable(predicted = predicted_response, observed = validation_data$churn, n_segments = 20)
kable(lift_DT, digits = 3)
ggplot(lift_DT, aes(x = score_group_no, y = avg_observed)) +
geom_line() +
geom_point() +
labs(
x = "Score Group",
y = "Observed Churn Rate",
title = "Observed Churn Rate by Score Group"
)
ggplot(lift_DT, aes(x = score_group_no, y = lift_factor * 100)) +
geom_line() +
geom_point() +
geom_hline(yintercept = 100, linetype = "dashed", color = "red") +
labs(
x = "Score Group",
y = "Lift Factor (%)",
title = "Lift Factor by Score Group"
)
standardize_columns <- function(x) {
# Check if the column is a dummy variable
elements = unique(x)
if (length(elements) == 2L & all(elements %in% c(0L,1L))) {
is_dummy = TRUE
} else {
is_dummy = FALSE
}
# If not a dummy, divide values in x by its standard deviation
if (is_dummy == FALSE) x = x/sd(x, na.rm = TRUE)
return(x)
}
class(1)
class(1L)
DT_lin_prob = cell2cell_DT[calibrat == 1]
# Create a vector that contains the names of all inputs (covariates)
all_columns   = names(DT_lin_prob)
input_columns = all_columns[-c(1:3, length(all_columns))]
# Standardize all input columns
DT_lin_prob[, (input_columns) := lapply(.SD, standardize_columns), .SDcols = input_columns]
#Q4
# Fit Linear Probability Model
predictors = setdiff(names(DT_lin_prob), c("churn", "customer", "calibrat"))
formula = as.formula(paste("churn ~", paste(predictors, collapse = " + ")))
# Fit the logistic regression model
fit = glm(formula, data = DT_lin_prob, family = binomial(link = "logit"))
# 3. Calculate effect sizes
# Extract coefficients using tidy() from the broom package
results_DT = as.data.table(tidy(fit))
# Calculate effect size using scaling formula
p_t = mean(training_data$churn)  # Churn rate in the training data
p_v = mean(validation_data$churn)  # Churn rate in the validation data
results_DT[, effect_size := 100 * estimate * (p_v / p_t)]
# Sort results by effect size and display the table
results_DT = results_DT[order(abs(effect_size), decreasing = TRUE)]
kable(results_DT, digits = 5)
# Goal: calculate LTV without churn program and LTV with churn program
# Step 1: Create groups based on predicted_churn ranking.
# Step 2: Create function calculates LTV without churn program (however, allow the function to take in a churn adjustment that we can plug gamma into) -> Sum for each year retention_rate^t * customer_profit divided by (1+.08)^t
# Step 3: Calculate this for all groups and store the values
# Step 4: Calculate this for all groups with churn adjustment of .25 and store the values
# Step 5: Calculate this for all groups with churn adjustment of .5 and store the values
# Key things to note: churn is monthly and so is revenue. Account for this fact.
create_n_groups <- function(predicted, observed, n_segments) {
DT <- data.table(predicted_churn = predicted, observed_churn = observed$churn, observed_revenue = observed$revenue)
DT[, score_group := cut_number(predicted, n = n_segments)]
DT[, score_group_no := as.integer(score_group)]
group_DT <- DT[, .(
avg_score = mean(predicted_churn),
avg_observed = 1 - (1 - mean(observed_churn))^12,
avg_annual_revenue = 12 * mean(observed_revenue)
), by = score_group_no]
return(group_DT);
}
#Create groups based on predicted churn with data on predicted churn, observed churn, and annual revenue. Churn is adjusted for yearly churn.
grouped_data = create_n_groups(predicted = predicted_response, observed = validation_data, n_segments=20)
calculate_LTV <- function(observed_churn, churn_program_success, annual_revenue, years, discount_rate) {
# Key assumption: predicting average per person revenue while accounting for churn
total_revenue = 0
adjusted_churn_rate = observed_churn * (1 - churn_program_success)
for(t in 1:years) {
retention_rate = (1 - adjusted_churn_rate)^t
discounted_cash_flow = (annual_revenue * retention_rate) / (1 + discount_rate)^t
total_revenue = total_revenue + discounted_cash_flow
}
return(total_revenue)
}
# Assuming years = 4, discount_rate = 8%
years <- 4
discount_rate <- 0.08
success_probabilities <- c(0.25, 0.5)
# Calculate LTV for each group and each gamma
grouped_data[, LTV_no_program := calculate_LTV(avg_observed, 0, avg_annual_revenue, years, discount_rate)]
for (gamma in success_probabilities) {
column_name <- paste("LTV_with_gamma", gamma, sep = "_")
grouped_data[, (column_name) := calculate_LTV(avg_observed, gamma, avg_annual_revenue, years, discount_rate)]
}
grouped_data = setorder(grouped_data, score_group_no)
grouped_data
data_Y1 = cell2cell_DT[churn == 1]
data_Y0 = cell2cell_DT[churn == 0]
training_data = calibration_data
validation_data = cell2cell_DT[calibrat == 0]
p_t = mean(training_data$churn)  # Oversampled rate
p_v = mean(validation_data$churn)  # True rate in the validation sample
offset_value = (log(p_t) - log(1 - p_t)) - (log(p_v) - log(1 - p_v))
training_data[, offset_var := offset_value]
predictors = setdiff(names(training_data), c("churn", "customer", "calibrat", "offset_var"))
formula = as.formula(paste("churn ~ offset(offset_var) +", paste(predictors, collapse = " + ")))
fit = lm(formula, data = DT_lin_prob)
data_Y1 = cell2cell_DT[churn == 1]
data_Y0 = cell2cell_DT[churn == 0]
training_data = calibration_data
validation_data = cell2cell_DT[calibrat == 0]
p_t = mean(training_data$churn)  # Oversampled rate
p_v = mean(validation_data$churn)  # True rate in the validation sample
offset_value = (log(p_t) - log(1 - p_t)) - (log(p_v) - log(1 - p_v))
training_data[, offset_var := offset_value]
predictors = setdiff(names(training_data), c("churn", "customer", "calibrat", "offset_var"))
formula = as.formula(paste("churn ~ offset(offset_var) +", paste(predictors, collapse = " + ")))
fit = glm(formula, data = training_data, family = binomial(link = "logit"))
results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
validation_data[, offset_var := 0]
predicted_response = predict(fit, newdata = validation_data, type = "response")
mean_predicted_response = mean(predicted_response)
mean_observed_response = mean(validation_data$churn)
list(mean_predicted = mean_predicted_response, mean_observed = mean_observed_response)
data_Y1 = cell2cell_DT[churn == 1]
data_Y0 = cell2cell_DT[churn == 0]
training_data = calibration_data
validation_data = cell2cell_DT[calibrat == 0]
p_t = mean(training_data$churn)  # Oversampled rate
p_v = mean(validation_data$churn)  # True rate in the validation sample
offset_value = (log(p_t) - log(1 - p_t)) - (log(p_v) - log(1 - p_v))
training_data[, offset_var := offset_value]
predictors = setdiff(names(training_data), c("churn", "customer", "calibrat", "offset_var"))
formula = as.formula(paste("churn ~ offset(offset_var) +", paste(predictors, collapse = " + ")))
# For linear probability model, create new formula without offset
predictors = setdiff(names(DT_lin_prob), c("churn", "customer", "calibrat", "offset_var"))
formula_lin = as.formula(paste("churn ~", paste(predictors, collapse = " + ")))
# Fit the linear probability model
fit = lm(formula_lin, data = DT_lin_prob)
results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
validation_data[, offset_var := 0]
predicted_response = predict(fit, newdata = validation_data, type = "response")
mean_predicted_response = mean(predicted_response)
mean_observed_response = mean(validation_data$churn)
list(mean_predicted = mean_predicted_response, mean_observed = mean_observed_response)
data_Y1 = cell2cell_DT[churn == 1]
data_Y0 = cell2cell_DT[churn == 0]
training_data = calibration_data
validation_data = cell2cell_DT[calibrat == 0]
p_t = mean(training_data$churn)  # Oversampled rate
p_v = mean(validation_data$churn)  # True rate in the validation sample
offset_value = (log(p_t) - log(1 - p_t)) - (log(p_v) - log(1 - p_v))
training_data[, offset_var := offset_value]
predictors = setdiff(names(training_data), c("churn", "customer", "calibrat", "offset_var"))
formula = as.formula(paste("churn ~ offset(offset_var) +", paste(predictors, collapse = " + ")))
fit = glm(formula, data = training_data, family = binomial(link = "logit"))
results_DT = as.data.table(tidy(fit))
kable(results_DT, digits = 5)
validation_data[, offset_var := 0]
predicted_response = predict(fit, newdata = validation_data, type = "response")
mean_predicted_response = mean(predicted_response)
mean_observed_response = mean(validation_data$churn)
list(mean_predicted = mean_predicted_response, mean_observed = mean_observed_response)
# Goal: calculate LTV without churn program and LTV with churn program
# Step 1: Create groups based on predicted_churn ranking.
# Step 2: Create function calculates LTV without churn program (however, allow the function to take in a churn adjustment that we can plug gamma into) -> Sum for each year retention_rate^t * customer_profit divided by (1+.08)^t
# Step 3: Calculate this for all groups and store the values
# Step 4: Calculate this for all groups with churn adjustment of .25 and store the values
# Step 5: Calculate this for all groups with churn adjustment of .5 and store the values
# Key things to note: churn is monthly and so is revenue. Account for this fact.
create_n_groups <- function(predicted, observed, n_segments) {
DT <- data.table(
predicted_churn = predicted,
observed_churn = observed$churn,
observed_revenue = observed$revenue
)
DT[, score_group := cut_number(predicted, n = n_segments)]
DT[, score_group_no := as.integer(score_group)]
group_DT <- DT[, .(
avg_score = mean(predicted_churn),
avg_monthly_churn = mean(observed_churn),
avg_annual_revenue = 12 * mean(observed_revenue)
), by = score_group_no]
# Calculate yearly churn rate
group_DT[, avg_yearly_churn := 1 - (1 - avg_monthly_churn)^12]
return(group_DT)
}
calculate_LTV <- function(monthly_churn, churn_program_success, annual_revenue,
years, discount_rate, program_cost = 0) {
# Convert monthly churn to yearly
yearly_churn = 1 - (1 - monthly_churn)^12
# Adjust churn rate based on program success
adjusted_yearly_churn = yearly_churn * (1 - churn_program_success)
total_revenue = -program_cost  # Include initial program cost
for(t in 1:years) {
retention_rate = (1 - adjusted_yearly_churn)^t
discounted_cash_flow = (annual_revenue * retention_rate) / (1 + discount_rate)^t
total_revenue = total_revenue + discounted_cash_flow
}
return(total_revenue)
}
# Assuming years = 4, discount_rate = 8%
years <- 4
discount_rate <- 0.08
success_probabilities <- c(0.25, 0.5)
# Calculate LTV for each group and each gamma
grouped_data[, LTV_no_program := calculate_LTV(avg_observed, 0, avg_annual_revenue, years, discount_rate)]
for (gamma in success_probabilities) {
column_name <- paste("LTV_with_gamma", gamma, sep = "_")
grouped_data[, (column_name) := calculate_LTV(avg_observed, gamma, avg_annual_revenue, years, discount_rate)]
}
